<!DOCTYPE html>
<html lang="en">

<!-- Head tag -->
<head><meta name="generator" content="Hexo 3.8.0">
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="google-site-verification" content="xBT4GhYoi5qRD5tr338pgPM5OWHHIDR6mNg1a3euekI">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <meta name="keyword" content="">
    <link rel="shortcut icon" href="/img/favicon.ico">

    <title>
        
          Python Machine Learning II - Master Gloomymoon&#39;s R2D2
        
    </title>

    <link rel="canonical" href="http://gloomymoon.github.io/2017/01/18/Python-Machine-Learning-II/">

    <!-- Bootstrap Core CSS -->
    <link rel="stylesheet" href="/css/bootstrap.min.css">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/hux-blog.css">

    <!-- Pygments Highlight CSS -->
    <link rel="stylesheet" href="/css/highlight.css">

    <!-- Custom Fonts -->
    <!-- <link href="http://maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css" rel="stylesheet" type="text/css"> -->
    <!-- Hux change font-awesome CDN to qiniu -->
    <link rel="stylesheet" href="/css/font-awesome.min.css">
    <link href="//cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css">


    <!-- Hux Delete, sad but pending in China
    <link href='http://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
    <link href='http://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800' rel='stylesheet' type='text/
    css'>
    -->


    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

    <!-- ga & ba script hoook -->
    <script></script>
</head>


<!-- hack iOS CSS :active style -->
<body ontouchstart="">

    <!-- Navigation -->
<nav class="navbar navbar-default navbar-custom navbar-fixed-top">
    <div class="container-fluid">
        <!-- Brand and toggle get grouped for better mobile display -->
        <div class="navbar-header page-scroll">
            <button type="button" class="navbar-toggle">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="/">Gloomymoon</a>
        </div>

        <!-- Collect the nav links, forms, and other content for toggling -->
        <!-- Known Issue, found by Hux:
            <nav>'s height woule be hold on by its content.
            so, when navbar scale out, the <nav> will cover tags.
            also mask any touch event of tags, unfortunately.
        -->
        <div id="huxblog_navbar">
            <div class="navbar-collapse">
                <ul class="nav navbar-nav navbar-right">
                    <li>
                        <a href="/">Home</a>
                    </li>

                    

                        
                        <li>
                            <a href="/about/">About</a>
                        </li>
                        
                    

                        
                        <li>
                            <a href="/tags/">Tags</a>
                        </li>
                        
                    

                        
                        <li>
                            <a href="/archives/">Archives</a>
                        </li>
                        
                    

                        
                    
                    
                </ul>
            </div>
        </div>
        <!-- /.navbar-collapse -->
    </div>
    <!-- /.container -->
</nav>
<script>
    // Drop Bootstarp low-performance Navbar
    // Use customize navbar with high-quality material design animation
    // in high-perf jank-free CSS3 implementation
    var $body   = document.body;
    var $toggle = document.querySelector('.navbar-toggle');
    var $navbar = document.querySelector('#huxblog_navbar');
    var $collapse = document.querySelector('.navbar-collapse');

    $toggle.addEventListener('click', handleMagic)
    function handleMagic(e){
        if ($navbar.className.indexOf('in') > 0) {
        // CLOSE
            $navbar.className = " ";
            // wait until animation end.
            setTimeout(function(){
                // prevent frequently toggle
                if($navbar.className.indexOf('in') < 0) {
                    $collapse.style.height = "0px"
                }
            },400)
        }else{
        // OPEN
            $collapse.style.height = "auto"
            $navbar.className += " in";
        }
    }
</script>


    <!-- Main Content -->
    
<!-- Image to hack wechat -->
<!-- <img src="http://gloomymoon.github.io/img/icon_wechat.png" width="0" height="0"> -->
<!-- <img src="{{ site.baseurl }}/{% if page.header-img %}{{ page.header-img }}{% else %}{{ site.header-img }}{% endif %}" width="0" height="0"> -->

<!-- Post Header -->
<style type="text/css">
    header.intro-header{
        background-image: url('/img/star-wars.jpg');
    }
</style>
<header class="intro-header">
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <div class="post-heading">
                    <div class="tags">
                        
                          <a class="tag" href="/tags/#Python" title="Python">Python</a>
                        
                          <a class="tag" href="/tags/#Machine Learning" title="Machine Learning">Machine Learning</a>
                        
                          <a class="tag" href="/tags/#scikit-learn" title="scikit-learn">scikit-learn</a>
                        
                    </div>
                    <h1>Python Machine Learning II</h1>
                    <h2 class="subheading"></h2>
                    <span class="meta">
                        Posted by Gloomymoon on
                        2017-01-18
                    </span>
                </div>
            </div>
        </div>
    </div>
</header>

<!-- Post Content -->
<article>
    <div class="container">
        <div class="row">

    <!-- Post Container -->
            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                post-container">

                <h2 id="2-Training-Machine-Learning-Algorithms-for-Classification"><a href="#2-Training-Machine-Learning-Algorithms-for-Classification" class="headerlink" title="2 Training Machine Learning Algorithms for Classification"></a>2 Training Machine Learning Algorithms for Classification</h2><ul>
<li>建立机器学习算法的基础知识</li>
<li>利用pandas、NumPy和matplotlib来读入、处理和展示数据</li>
<li>用Python实现线性分类算法</li>
</ul>
<h3 id="Artificial-neurons-–-a-brief-glimpse-into-the-early-history-of-machine-learning"><a href="#Artificial-neurons-–-a-brief-glimpse-into-the-early-history-of-machine-learning" class="headerlink" title="Artificial neurons – a brief glimpse into the early history of machine learning"></a>Artificial neurons – a brief glimpse into the early history of machine learning</h3><p>早期大脑神经元结构对人工智能研究的影响：<br><img src="/img/PythonMachineLearningII_01.png" alt=""></p>
<p>每个神经元就是一个二元分类器，定义一个激活函数Φ(z)来实现分类，其中z是一系列输入变量x的线性组合z = w1x1+w2x2+…+wmxm，其中w表示每个变量的权重：<br><img src="/img/PythonMachineLearningII_02.png" alt=""></p>
<p>下图描绘了输入是如何转换成二元输出的示意图：<br><img src="/img/PythonMachineLearningII_03.png" alt=""></p>
<p>整个感知器的训练过程如下：</p>
<ol>
<li>所有w初始化为0或者一个很小的随机数</li>
<li>对于每个训练样本x，计算输出y，并基于结果与否更新w，更新的变化值如下：<br><img src="/img/PythonMachineLearningII_04.png" alt=""></li>
</ol>
<p>只有当分类结果是线性且学习率充分小时，感知器才能够保证收敛。<br><img src="/img/PythonMachineLearningII_05.png" alt=""></p>
<p>感知器概念图，在学习阶段，激活函数的输出用来更新每个输入变量的权重参数。</p>
<h3 id="Implementing-a-perceptron-learning-algorithm-in-Python"><a href="#Implementing-a-perceptron-learning-algorithm-in-Python" class="headerlink" title="Implementing a perceptron learning algorithm in Python"></a>Implementing a perceptron learning algorithm in Python</h3><p>Perceptron.py<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Perceptron</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""Perceptron classifier.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    eta : float</span></span><br><span class="line"><span class="string">        Learning rate (between 0.0 and 1.0)</span></span><br><span class="line"><span class="string">    n_iter : int</span></span><br><span class="line"><span class="string">        Passes over the traning dataset.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Attributes</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    w_ : 1d-array</span></span><br><span class="line"><span class="string">        Weights after fitting.</span></span><br><span class="line"><span class="string">    errors_ : list</span></span><br><span class="line"><span class="string">        Number of misclassifications in every epoch.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, eta=<span class="number">0.01</span>, n_iter=<span class="number">10</span>)</span>:</span></span><br><span class="line">        self.eta = eta</span><br><span class="line">        self.n_iter = n_iter</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, X, y)</span>:</span></span><br><span class="line">        <span class="string">"""Fit training data.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Parameters</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">        X : &#123;array-like&#125;, shape = [n_samples, n_features]</span></span><br><span class="line"><span class="string">            Training vectors, where n_samples is the number of samples </span></span><br><span class="line"><span class="string">            and n_features is the number of features.</span></span><br><span class="line"><span class="string">        y : array-like, shape = [n_samples]</span></span><br><span class="line"><span class="string">            Target values.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns</span></span><br><span class="line"><span class="string">        -------</span></span><br><span class="line"><span class="string">        self : object</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.w_ = np.zeros(<span class="number">1</span> + X.shape[<span class="number">1</span>])</span><br><span class="line">        self.errors_ = []</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> range(self.n_iter):</span><br><span class="line">            errors = <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> xi, target <span class="keyword">in</span> zip(X, y):</span><br><span class="line">                update = self.eta * (target - self.predict(xi))</span><br><span class="line">                self.w_[<span class="number">1</span>:] += update * xi</span><br><span class="line">                self.w_[<span class="number">0</span>] += update</span><br><span class="line">                errors += int(update != <span class="number">0.0</span>)</span><br><span class="line">            self.errors_.append(errors)</span><br><span class="line">        <span class="keyword">return</span> self</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">net_input</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="string">"""Calculate net input"""</span></span><br><span class="line">        <span class="keyword">return</span> np.dot(X, self.w_[<span class="number">1</span>:]) + self.w_[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="string">"""Return class label after unit step"""</span></span><br><span class="line">        <span class="keyword">return</span> np.where(self.net_input(X) &gt;= <span class="number">0.0</span>, <span class="number">1</span>, <span class="number">-1</span>)</span><br></pre></td></tr></table></figure></p>
<p><strong>Training a perceptron model on the Iris dataset</strong><br>为测试感知器，我们使用Iris中的Setosa和Versicolor两类数据，便于展示这里仅使用sepal length和petal length两个变量。<br><figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="title">from</span> perceptron <span class="keyword">import</span> Perceptron</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="title">df</span> = pd.read_csv('iris.<span class="class"><span class="keyword">data</span>', header=<span class="type">None</span>)</span></span><br><span class="line"><span class="title">df</span>.head()</span><br></pre></td></tr></table></figure></p>
<p>Output:<br><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">     <span class="number">0</span>      <span class="number">1</span>      <span class="number">2</span>      <span class="number">3</span>      <span class="number">4</span></span><br><span class="line"><span class="number">0</span>    <span class="number">5.1</span>    <span class="number">3.5</span>    <span class="number">1.4</span>    <span class="number">0.2</span>    Iris-setosa</span><br><span class="line"><span class="number">1</span>    <span class="number">4.9</span>    <span class="number">3.0</span>    <span class="number">1.4</span>    <span class="number">0.2</span>    Iris-setosa</span><br><span class="line"><span class="number">2</span>    <span class="number">4.7</span>    <span class="number">3.2</span>    <span class="number">1.3</span>    <span class="number">0.2</span>    Iris-setosa</span><br><span class="line"><span class="number">3</span>    <span class="number">4.6</span>    <span class="number">3.1</span>    <span class="number">1.5</span>    <span class="number">0.2</span>    Iris-setosa</span><br><span class="line"><span class="number">4</span>    <span class="number">5.0</span>    <span class="number">3.6</span>    <span class="number">1.4</span>    <span class="number">0.2</span>    Iris-setosa</span><br></pre></td></tr></table></figure></p>
<figure class="highlight xl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">%pylab inline</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">y = df.iloc[<span class="number">0</span>:<span class="number">100</span>, <span class="number">4</span>].values</span><br><span class="line">y = np.<span class="keyword">where</span>(y == <span class="string">'Iris-setosa'</span>, -<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">X = df.iloc[<span class="number">0</span>:<span class="number">100</span>, [<span class="number">0</span>, <span class="number">2</span>]].values</span><br><span class="line">plt.scatter(X[:<span class="number">50</span>, <span class="number">0</span>], X[:<span class="number">50</span>, <span class="number">1</span>], <span class="built_in">color</span>=<span class="string">'red'</span>, marker=<span class="string">'o'</span>, label=<span class="string">'setosa'</span>)</span><br><span class="line">plt.scatter(X[<span class="number">50</span>:<span class="number">100</span>, <span class="number">0</span>], X[<span class="number">50</span>:<span class="number">100</span>, <span class="number">1</span>], <span class="built_in">color</span>=<span class="string">'blue'</span>, marker=<span class="string">'x'</span>, label=<span class="string">'versicolor'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'petal length'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'sepal length'</span>)</span><br><span class="line">plt.legend(loc=<span class="string">'upper left'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/img/PythonMachineLearningII_06.png" alt=""></p>
<p>现在可以用我们的感知器来训练Iris数据<br><figure class="highlight ada"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">ppn = Perceptron(eta=<span class="number">0.1</span>, n_iter=<span class="number">10</span>)</span><br><span class="line">ppn.fit(X, y)</span><br><span class="line">plt.plot(<span class="keyword">range</span>(<span class="number">1</span>, len(ppn.errors_) + <span class="number">1</span>), ppn.errors_, marker=<span class="string">'o'</span>)</span><br><span class="line">plt.xlabel(<span class="symbol">'Epochs</span>')</span><br><span class="line">plt.ylabel(<span class="symbol">'Number</span> <span class="keyword">of</span> misclassifications')</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p>
<p><img src="/img/PythonMachineLearningII_07.png" alt=""></p>
<p>六次迭代后感知器已经收敛。<br>为了便于展现编写了一个2维数据可视化函数plot_decision_regions。<br><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">from matplotlib.colors import ListedColormap</span><br><span class="line"></span><br><span class="line">def plot_decision_regions(<span class="keyword">X</span>, <span class="keyword">y</span>, classifier, resolution=<span class="number">0.02</span>):</span><br><span class="line">    # setup marker generator <span class="built_in">and</span> color <span class="keyword">map</span></span><br><span class="line">    markers = (<span class="string">'s'</span>, <span class="string">'x'</span>, <span class="string">'o'</span>, <span class="string">'^'</span>, <span class="string">'v'</span>)</span><br><span class="line">    colors = (<span class="string">'red'</span>, <span class="string">'blue'</span>, <span class="string">'lightgreen'</span>, <span class="string">'gray'</span>, <span class="string">'cyan'</span>)</span><br><span class="line">    <span class="keyword">cmap</span> = ListedColormap(colors[:<span class="built_in">len</span>(np.unique(<span class="keyword">y</span>))])</span><br><span class="line"></span><br><span class="line">    # plot the decision surface</span><br><span class="line">    x1_min, x1_max = <span class="keyword">X</span>[:, <span class="number">0</span>].<span class="built_in">min</span>() - <span class="number">1</span>, <span class="keyword">X</span>[:, <span class="number">0</span>].<span class="built_in">max</span>() + <span class="number">1</span></span><br><span class="line">    x2_min, x2_max = <span class="keyword">X</span>[:, <span class="number">1</span>].<span class="built_in">min</span>() - <span class="number">1</span>, <span class="keyword">X</span>[:, <span class="number">1</span>].<span class="built_in">max</span>() + <span class="number">1</span></span><br><span class="line">    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution), np.arange(x2_min, x2_max, resolution))</span><br><span class="line">    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)</span><br><span class="line">    Z = Z.reshape(xx1.shape)</span><br><span class="line">    plt.contourf(xx1, xx2, Z, alpha=<span class="number">0.4</span>, <span class="keyword">cmap</span>=<span class="keyword">cmap</span>)</span><br><span class="line">    plt.xlim(xx1.<span class="built_in">min</span>(), xx1.<span class="built_in">max</span>())</span><br><span class="line">    plt.ylim(xx2.<span class="built_in">min</span>(), xx2.<span class="built_in">max</span>())</span><br><span class="line"></span><br><span class="line">    # plot class samples</span><br><span class="line">    <span class="keyword">for</span> idx, <span class="keyword">cl</span> in enumerate(np.unique(<span class="keyword">y</span>)):</span><br><span class="line">        plt.scatter(<span class="keyword">x</span>=<span class="keyword">X</span>[<span class="keyword">y</span> == <span class="keyword">cl</span>, <span class="number">0</span>], <span class="keyword">y</span>=<span class="keyword">X</span>[<span class="keyword">y</span> == <span class="keyword">cl</span>, <span class="number">1</span>], </span><br><span class="line">                    alpha=<span class="number">0.8</span>, <span class="keyword">c</span>=<span class="keyword">cmap</span>(idx), </span><br><span class="line">                    marker=markers[idx], label=<span class="keyword">cl</span>)</span><br></pre></td></tr></table></figure></p>
<p>然后就可以展现感知器的分类线，能够完美区分样本中的Iris类型。<br><figure class="highlight less"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">plot_decision_regions</span>(X, y, classifier=ppn)</span><br><span class="line"><span class="selector-tag">plt</span><span class="selector-class">.xlabel</span>(<span class="string">'sepal length [cm]'</span>)</span><br><span class="line"><span class="selector-tag">plt</span><span class="selector-class">.ylabel</span>(<span class="string">'petal length [cm]'</span>)</span><br><span class="line"><span class="selector-tag">plt</span><span class="selector-class">.legend</span>(loc=<span class="string">'upper left'</span>)</span><br><span class="line"><span class="selector-tag">plt</span><span class="selector-class">.show</span>()</span><br></pre></td></tr></table></figure></p>
<p><img src="/img/PythonMachineLearningII_08.png" alt=""></p>
<h3 id="Adaptive-linear-neurons-and-the-convergence-of-learning"><a href="#Adaptive-linear-neurons-and-the-convergence-of-learning" class="headerlink" title="Adaptive linear neurons and the convergence of learning"></a>Adaptive linear neurons and the convergence of learning</h3><p>ADAptive Linear Neuron(Adaline)是对感知器算法的一个改进，其关键理念是定义并最小化成本函数，基于线性激活函数替代单步阶梯函数。<br><img src="/img/PythonMachineLearningII_09.png" alt=""></p>
<p><strong>Minimizing cost functions with gradient descent</strong><br>有监督学习算法的关键因素之一就是定义一个目标函数（objective function）。目标函数通常可以是最小化的陈本函数，在Adaline方法中，成本函数J定义为输出和目标的误差平方和（Sum of Squared Errors, SSE），相比单步阶梯函数该线性激活函数是可微的：<br><img src="/img/PythonMachineLearningII_10.png" alt=""></p>
<p>另一个优势是这是个凸函数（convex），因此可以使用一个简单强大的优化算法，梯度下降（gradient descent），找到成本函数最小的权重值。梯度下降算法示意图如下：<br><img src="/img/PythonMachineLearningII_11.png" alt=""></p>
<p><strong>Implementing an Adaptive Linear Neuron in Python</strong><br>Adaline和感知器很类似，所以可以直接在原有的代码上修改<code>fit</code>方法重写成本函数最小化算法。<br>AdalineGD.py<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AdalineGD</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""ADAptive LInear NEuron classifier.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    eta : float</span></span><br><span class="line"><span class="string">        Learning rate (between 0.0 and 1.0)</span></span><br><span class="line"><span class="string">    n_iter : int</span></span><br><span class="line"><span class="string">        Passes over the traning dataset.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Attributes</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    w_ : 1d-array</span></span><br><span class="line"><span class="string">        Weights after fitting.</span></span><br><span class="line"><span class="string">    errors_ : list</span></span><br><span class="line"><span class="string">        Number of misclassifications in every epoch.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, eta=<span class="number">0.01</span>, n_iter=<span class="number">10</span>)</span>:</span></span><br><span class="line">        self.eta = eta</span><br><span class="line">        self.n_iter = n_iter</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, X, y)</span>:</span></span><br><span class="line">        <span class="string">"""Fit training data.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Parameters</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">        X : &#123;array-like&#125;, shape = [n_samples, n_features]</span></span><br><span class="line"><span class="string">            Training vectors, where n_samples is the number of samples </span></span><br><span class="line"><span class="string">            and n_features is the number of features.</span></span><br><span class="line"><span class="string">        y : array-like, shape = [n_samples]</span></span><br><span class="line"><span class="string">            Target values.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns</span></span><br><span class="line"><span class="string">        -------</span></span><br><span class="line"><span class="string">        self : object</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.w_ = np.zeros(<span class="number">1</span> + X.shape[<span class="number">1</span>])</span><br><span class="line">        self.cost_ = []</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(self.n_iter):</span><br><span class="line">            output = self.net_input(X)</span><br><span class="line">            errors = (y - output)</span><br><span class="line">            self.w_[<span class="number">1</span>:] += self.eta * X.T.dot(errors)</span><br><span class="line">            self.w_[<span class="number">0</span>] += self.eta * errors.sum()</span><br><span class="line">            cost = (errors**<span class="number">2</span>).sum() / <span class="number">2.0</span></span><br><span class="line">            self.cost_.append(cost)</span><br><span class="line">        <span class="keyword">return</span> self</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">net_input</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="string">"""Calculate net input"""</span></span><br><span class="line">        <span class="keyword">return</span> np.dot(X, self.w_[<span class="number">1</span>:]) + self.w_[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">activation</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="string">"""Compute linear activation"""</span></span><br><span class="line">        <span class="keyword">return</span> self.net_input(X)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="string">"""Return class label after unit step"""</span></span><br><span class="line">        <span class="keyword">return</span> np.where(self.activation(X) &gt;= <span class="number">0.0</span>, <span class="number">1</span>, <span class="number">-1</span>)</span><br></pre></td></tr></table></figure></p>
<p>实践中，经常需要尝试不同的学习率使结果收敛。这里我们尝试0.1和0.0001两个参数进行比较。<br><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">from AdalineGD import AdalineGD</span><br><span class="line">fig, ax = plt.subplots(nrows=<span class="number">1</span>, ncols=<span class="number">2</span>, figsize=(<span class="number">8</span>, <span class="number">4</span>))</span><br><span class="line">ada1 = AdalineGD(n_iter=<span class="number">10</span>, eta=<span class="number">0.01</span>).fit(X, y)</span><br><span class="line">ax[<span class="number">0</span>].plot(range(<span class="number">1</span>, len(ada1.cost_) + <span class="number">1</span>), np.log10(ada1.cost_), marker='o')</span><br><span class="line">ax[<span class="number">0</span>].set_xlabel('Epochs')</span><br><span class="line">ax[<span class="number">0</span>].set_ylabel('log(Sum-squared-error)')</span><br><span class="line">ax[<span class="number">0</span>].set_title('Adaline - Learning rate <span class="number">0.01</span>')</span><br><span class="line">ada2 = AdalineGD(n_iter=<span class="number">10</span>, eta=<span class="number">0.0001</span>).fit(X, y)</span><br><span class="line">ax[<span class="number">1</span>].plot(range(<span class="number">1</span>, len(ada2.cost_) + <span class="number">1</span>), ada2.cost_, marker='o')</span><br><span class="line">ax[<span class="number">1</span>].set_xlabel('Epochs')</span><br><span class="line">ax[<span class="number">1</span>].set_ylabel('Sum-squared-error')</span><br><span class="line">ax[<span class="number">1</span>].set_title('Adaline - Learning rate <span class="number">0.0001</span>')</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p>
<p><img src="/img/PythonMachineLearningII_12.png" alt=""></p>
<p>这两次实践分别遇到了两类典型的问题，第一次（左图）选用的学习率太大，算法在迭代中直接越过最小值最后发散，第二次（右图）选取的学习率太低，多次迭代后还没有达到收敛。</p>
<p>许多机器学习算法的调优都需要先进行特征归一化，梯度下降是其中之一。这里我们使用标准化（standardization）方法进行特征归一化，使数据分布正态化，即每个特征的以0为中心方差为1的正态分布。然后再使用0.01的学习率来训练Adaline算法。<br><figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">X_std = <span class="built_in">np</span>.<span class="built_in">copy</span>(X)</span><br><span class="line">X_std[:, <span class="number">0</span>] = (X[:, <span class="number">0</span>] - X[:, <span class="number">0</span>].<span class="built_in">mean</span>()) / X[:, <span class="number">0</span>].<span class="built_in">std</span>()</span><br><span class="line">X_std[:, <span class="number">1</span>] = (X[:, <span class="number">1</span>] - X[:, <span class="number">1</span>].<span class="built_in">mean</span>()) / X[:, <span class="number">1</span>].<span class="built_in">std</span>()</span><br><span class="line"></span><br><span class="line">ada = AdalineGD(n_iter=<span class="number">15</span>, eta=<span class="number">0.01</span>)</span><br><span class="line">ada.fit(X_std, y)</span><br><span class="line">plot_decision_regions(X_std, y, classifier=ada)</span><br><span class="line">plt.<span class="built_in">title</span>('Adaline - Gradient Descent')</span><br><span class="line">plt.<span class="built_in">xlabel</span>('sepal <span class="built_in">length</span> [standardized]')</span><br><span class="line">plt.<span class="built_in">ylabel</span>('petal <span class="built_in">length</span> [standardized]')</span><br><span class="line">plt.<span class="built_in">legend</span>(loc='upper left')</span><br><span class="line">plt.<span class="built_in">show</span>()</span><br><span class="line">plt.plot(<span class="built_in">range</span>(<span class="number">1</span>, len(ada.cost_) + <span class="number">1</span>), ada.cost_, marker='o')</span><br><span class="line">plt.<span class="built_in">xlabel</span>('Epochs')</span><br><span class="line">plt.<span class="built_in">ylabel</span>('Sum-squared-<span class="built_in">error</span>')</span><br><span class="line">plt.<span class="built_in">show</span>()</span><br></pre></td></tr></table></figure></p>
<p><img src="/img/PythonMachineLearningII_13.png" alt=""></p>
<p>Adaline收敛，但是注意尽管所有样本都正确分类，SSE仍然不是0。</p>
<p><strong>Large scale machine learning and stochastic gradient descent</strong><br>现实中用梯度下降算法在百万条数据的训练非常耗时，因为每次迭代都要对全量数据进行评估并更新权重参数来达到全局最小值。一个常用的替代方案是随机梯度下降（stochastic gradient descent），权重的更新不是基于所有样本的偏差累计，而是基于每个样本的偏差，使得算法能够更频繁的更新权重参数从而快速达到收敛。为了达到准确结果，使用随机梯度下降算法每次迭代前都需要对数据进行随机排序（shuffle）。<br>随机梯度下降的另一个优势是可以在线学习（online learning），随着系统运行，新的训练数据不断产生，在线学习可以快速适应变化，而且若无必要保存，训练数据使用后可以直接丢弃。<br>我们在AdalineGD上修改<code>fit</code>方法的权重更新方式，新增<code>partial_fit</code>方法用于在线学习（不重新初始化权重），新增<code>shuffle</code>参数和<code>random_state</code>参数用于。<br>AdalineSGD.py<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding=utf-8</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> numpy.random <span class="keyword">import</span> seed</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AdalineSGD</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""ADAptive LInear NEuron classifier.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    eta : float</span></span><br><span class="line"><span class="string">        Learning rate (between 0.0 and 1.0)</span></span><br><span class="line"><span class="string">    n_iter : int</span></span><br><span class="line"><span class="string">        Passes over the traning dataset.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Attributes</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    w_ : 1d-array</span></span><br><span class="line"><span class="string">        Weights after fitting.</span></span><br><span class="line"><span class="string">    errors_ : list</span></span><br><span class="line"><span class="string">        Number of misclassifications in every epoch.</span></span><br><span class="line"><span class="string">    shuffle : bool (default: True)</span></span><br><span class="line"><span class="string">        Shuffles traning data every epoch if True to prevent cycles.</span></span><br><span class="line"><span class="string">    random_state : int (default: None)</span></span><br><span class="line"><span class="string">        Set random state for shuffling and initializing the weights. </span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, eta=<span class="number">0.01</span>, n_iter=<span class="number">10</span>, shuffle=True, random_state=None)</span>:</span></span><br><span class="line">        self.eta = eta</span><br><span class="line">        self.n_iter = n_iter</span><br><span class="line">        self.w_initialized = <span class="keyword">False</span></span><br><span class="line">        self.shuffle = shuffle</span><br><span class="line">        <span class="keyword">if</span> random_state:</span><br><span class="line">            seed(random_state)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, X, y)</span>:</span></span><br><span class="line">        <span class="string">"""Fit training data.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Parameters</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">        X : &#123;array-like&#125;, shape = [n_samples, n_features]</span></span><br><span class="line"><span class="string">            Training vectors, where n_samples is the number of samples </span></span><br><span class="line"><span class="string">            and n_features is the number of features.</span></span><br><span class="line"><span class="string">        y : array-like, shape = [n_samples]</span></span><br><span class="line"><span class="string">            Target values.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns</span></span><br><span class="line"><span class="string">        -------</span></span><br><span class="line"><span class="string">        self : object</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self._initialize_weights(X.shape[<span class="number">1</span>])</span><br><span class="line">        self.cost_ = []</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(self.n_iter):</span><br><span class="line">            <span class="keyword">if</span> self.shuffle:</span><br><span class="line">                X, y = self._shuffle(X, y)</span><br><span class="line">            cost = []</span><br><span class="line">            <span class="keyword">for</span> xi, target <span class="keyword">in</span> zip(X, y):</span><br><span class="line">                cost.append(self._update_weights(xi, target))</span><br><span class="line">            avg_cost = sum(cost)/len(y)</span><br><span class="line">            self.cost_.append(avg_cost)</span><br><span class="line">        <span class="keyword">return</span> self</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">partial_fit</span><span class="params">(self, X, y)</span>:</span></span><br><span class="line">        <span class="string">"""Fit training data without reinitializing the weights"""</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.w_initialized:</span><br><span class="line">            self._initialize_weights(X.shape[<span class="number">1</span>]) <span class="comment"># ndarray.shap 返回一个维度列表, 列表长度等于维数(ndarray.ndim)</span></span><br><span class="line">        <span class="keyword">if</span> y.ravel().shape[<span class="number">0</span>] &gt; <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">for</span> xi, target <span class="keyword">in</span> zip(X, y):</span><br><span class="line">                self._update_weights(xi, target)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">                self._update_weights(X, y)</span><br><span class="line">        <span class="keyword">return</span> self</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_shuffle</span><span class="params">(self, X, y)</span>:</span></span><br><span class="line">        <span class="string">"""Shuffle training data"""</span></span><br><span class="line">        r = np.random.permutation(len(y)) <span class="comment"># permutation若传入一个整数，返回一个洗牌后的arange</span></span><br><span class="line">        <span class="keyword">return</span> X[r], y[r]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_initialize_weights</span><span class="params">(self, m)</span>:</span></span><br><span class="line">        <span class="string">"""Initialize weights to zeros"""</span></span><br><span class="line">        self.w_ = np.zeros(<span class="number">1</span> + m)</span><br><span class="line">        self.w_initialized = <span class="keyword">True</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_update_weights</span><span class="params">(self, xi, target)</span>:</span></span><br><span class="line">        <span class="string">"""Apply Adaline learning rule to update the weights"""</span></span><br><span class="line">        output = self.net_input(xi)</span><br><span class="line">        error = (target - output)</span><br><span class="line">        self.w_[<span class="number">1</span>:] += self.eta * xi.dot(error)</span><br><span class="line">        self.w_[<span class="number">0</span>] += self.eta * error</span><br><span class="line">        cost = <span class="number">0.5</span> * error**<span class="number">2</span></span><br><span class="line">        <span class="keyword">return</span> cost</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">net_input</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="string">"""Calculate net input"""</span></span><br><span class="line">        <span class="keyword">return</span> np.dot(X, self.w_[<span class="number">1</span>:]) + self.w_[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">activation</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="string">"""Compute linear activation"""</span></span><br><span class="line">        <span class="keyword">return</span> self.net_input(X)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="string">"""Return class label after unit step"""</span></span><br><span class="line">        <span class="keyword">return</span> np.where(self.activation(X) &gt;= <span class="number">0.0</span>, <span class="number">1</span>, <span class="number">-1</span>)</span><br></pre></td></tr></table></figure></p>
<p>使用AdalineSGD：<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">from AdalineSGD import AdalineSGD</span><br><span class="line"></span><br><span class="line">ada = AdalineSGD(n_iter=<span class="number">15</span>, eta=<span class="number">0.01</span>, random_state=<span class="number">1</span>)</span><br><span class="line">ada.fit(X_std, y)</span><br><span class="line"><span class="function"><span class="title">plot_decision_regions</span><span class="params">(X_std, y, classifier=ada)</span></span></span><br><span class="line">plt.title(<span class="string">'Adaline - Stochastic Gradient Descent'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'sepal length [standardized]'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'petal length [standardized]'</span>)</span><br><span class="line">plt.legend(loc=<span class="string">'upper left'</span>)</span><br><span class="line">plt.show()</span><br><span class="line">plt.plot(range(<span class="number">1</span>, len(ada.cost_) + <span class="number">1</span>), ada<span class="selector-class">.cost_</span>, marker=<span class="string">'o'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Epochs'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Average Cost'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p>
<p>从结果可以看到平均成本下降非常迅速，15次迭代后效果和梯度下降差不多。<br><img src="/img/PythonMachineLearningII_14.png" alt=""></p>
<p>如果需要在在线环境下基于流数据进行可以调用<code>partial_fit</code>方法针对每条记录进行训练。用法如下：<code>ada.partial_fit(X_std[0, :], y[0])</code></p>


                <hr>

                

                <ul class="pager">
                    
                        <li class="previous">
                            <a href="/2017/01/21/Python-Machine-Learning-III/" data-toggle="tooltip" data-placement="top" title="Python Machine Learning III">&larr; Previous Post</a>
                        </li>
                    
                    
                        <li class="next">
                            <a href="/2017/01/17/Python-Machine-Learning-I/" data-toggle="tooltip" data-placement="top" title="Python Machine Learning I">Next Post &rarr;</a>
                        </li>
                    
                </ul>

                

                

            </div>
    <!-- Side Catalog Container -->
        

    <!-- Sidebar Container -->

            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                sidebar-container">

                <!-- Featured Tags -->
                
                <section>
                    <!-- no hr -->
                    <h5><a href="/tags/">FEATURED TAGS</a></h5>
                    <div class="tags">
                       
                          <a class="tag" href="/tags/#Python" title="Python">Python</a>
                        
                          <a class="tag" href="/tags/#Machine Learning" title="Machine Learning">Machine Learning</a>
                        
                          <a class="tag" href="/tags/#scikit-learn" title="scikit-learn">scikit-learn</a>
                        
                    </div>
                </section>
                

                <!-- Friends Blog -->
                
            </div>

        </div>
    </div>
</article>







<!-- async load function -->
<script>
    function async(u, c) {
      var d = document, t = 'script',
          o = d.createElement(t),
          s = d.getElementsByTagName(t)[0];
      o.src = u;
      if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
      s.parentNode.insertBefore(o, s);
    }
</script>
<!-- anchor-js, Doc:http://bryanbraun.github.io/anchorjs/ -->
<script>
    async("//cdnjs.cloudflare.com/ajax/libs/anchor-js/1.1.1/anchor.min.js",function(){
        anchors.options = {
          visible: 'always',
          placement: 'right',
          icon: '#'
        };
        anchors.add().remove('.intro-header h1').remove('.subheading').remove('.sidebar-container h5');
    })
</script>
<style>
    /* place left on bigger screen */
    @media all and (min-width: 800px) {
        .anchorjs-link{
            position: absolute;
            left: -0.75em;
            font-size: 1.1em;
            margin-top : -0.1em;
        }
    }
</style>



    <!-- Footer -->
    <!-- Footer -->
<footer>
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <ul class="list-inline text-center">
                
                
                

                

                

                
                    <li>
                        <a target="_blank" href="https://github.com/Gloomymoon">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                

                

                </ul>
                <p class="copyright text-muted">
                    Copyright &copy; Gloomymoon 2021
                    <br>
                    Theme by <a href="http://huangxuan.me">Hux</a>
                    <span style="display: inline-block; margin: 0 5px;">
                        <i class="fa fa-heart"></i>
                    </span>
                    Ported by <a href="http://blog.kaijun.rocks">Kaijun</a> |
                    <iframe style="margin-left: 2px; margin-bottom:-5px;" frameborder="0" scrolling="0" width="91px" height="20px" src="https://ghbtns.com/github-btn.html?user=kaijun&repo=hexo-theme-huxblog&type=star&count=true">
                    </iframe>
                </p>
            </div>
        </div>
    </div>
</footer>

<!-- jQuery -->
<script src="/js/jquery.min.js"></script>

<!-- Bootstrap Core JavaScript -->
<script src="/js/bootstrap.min.js"></script>

<!-- Custom Theme JavaScript -->
<script src="/js/hux-blog.min.js"></script>


<!-- async load function -->
<script>
    function async(u, c) {
      var d = document, t = 'script',
          o = d.createElement(t),
          s = d.getElementsByTagName(t)[0];
      o.src = u;
      if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
      s.parentNode.insertBefore(o, s);
    }
</script>

<!--
     Because of the native support for backtick-style fenced code blocks
     right within the Markdown is landed in Github Pages,
     From V1.6, There is no need for Highlight.js,
     so Huxblog drops it officially.

     - https://github.com/blog/2100-github-pages-now-faster-and-simpler-with-jekyll-3-0
     - https://help.github.com/articles/creating-and-highlighting-code-blocks/
-->
<!--
    <script>
        async("http://cdn.bootcss.com/highlight.js/8.6/highlight.min.js", function(){
            hljs.initHighlightingOnLoad();
        })
    </script>
    <link href="http://cdn.bootcss.com/highlight.js/8.6/styles/github.min.css" rel="stylesheet">
-->


<!-- jquery.tagcloud.js -->
<script>
    // only load tagcloud.js in tag.html
    if($('#tag_cloud').length !== 0){
        async("http://gloomymoon.github.io/js/jquery.tagcloud.js",function(){
            $.fn.tagcloud.defaults = {
                //size: {start: 1, end: 1, unit: 'em'},
                color: {start: '#bbbbee', end: '#0085a1'},
            };
            $('#tag_cloud a').tagcloud();
        })
    }
</script>

<!--fastClick.js -->
<script>
    async("//cdnjs.cloudflare.com/ajax/libs/fastclick/1.0.6/fastclick.min.js", function(){
        var $nav = document.querySelector("nav");
        if($nav) FastClick.attach($nav);
    })
</script>


<!-- Google Analytics -->




<!-- Baidu Tongji -->


<!-- Side Catalog -->




<!-- Image to hack wechat -->
<img src="http://gloomymoon.github.io/img/icon_wechat.png" width="0" height="0" />
<!-- Migrate from head to bottom, no longer block render and still work -->

</body>

</html>
