<!DOCTYPE html>
<html lang="en">

<!-- Head tag -->
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="google-site-verification" content="xBT4GhYoi5qRD5tr338pgPM5OWHHIDR6mNg1a3euekI" />
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <meta name="keyword"  content="">
    <link rel="shortcut icon" href="/img/favicon.ico">

    <title>
        
          Python Machine Learning IV - Master Gloomymoon&#39;s R2D2
        
    </title>

    <link rel="canonical" href="http://gloomymoon.github.io/2017/01/25/Python-Machine-Learning-IV/">

    <!-- Bootstrap Core CSS -->
    <link rel="stylesheet" href="/css/bootstrap.min.css">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/hux-blog.css">

    <!-- Pygments Highlight CSS -->
    <link rel="stylesheet" href="/css/highlight.css">

    <!-- Custom Fonts -->
    <!-- <link href="http://maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css" rel="stylesheet" type="text/css"> -->
    <!-- Hux change font-awesome CDN to qiniu -->
    <link rel="stylesheet" href="/css/font-awesome.min.css">
    <link href="//cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css">


    <!-- Hux Delete, sad but pending in China
    <link href='http://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
    <link href='http://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800' rel='stylesheet' type='text/
    css'>
    -->


    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

    <!-- ga & ba script hoook -->
    <script></script>
</head>


<!-- hack iOS CSS :active style -->
<body ontouchstart="">

    <!-- Navigation -->
<nav class="navbar navbar-default navbar-custom navbar-fixed-top">
    <div class="container-fluid">
        <!-- Brand and toggle get grouped for better mobile display -->
        <div class="navbar-header page-scroll">
            <button type="button" class="navbar-toggle">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="/">Gloomymoon</a>
        </div>

        <!-- Collect the nav links, forms, and other content for toggling -->
        <!-- Known Issue, found by Hux:
            <nav>'s height woule be hold on by its content.
            so, when navbar scale out, the <nav> will cover tags.
            also mask any touch event of tags, unfortunately.
        -->
        <div id="huxblog_navbar">
            <div class="navbar-collapse">
                <ul class="nav navbar-nav navbar-right">
                    <li>
                        <a href="/">Home</a>
                    </li>

                    

                        
                    

                        
                        <li>
                            <a href="/about/">About</a>
                        </li>
                        
                    

                        
                        <li>
                            <a href="/archives/">Archives</a>
                        </li>
                        
                    

                        
                        <li>
                            <a href="/tags/">Tags</a>
                        </li>
                        
                    
                    
                </ul>
            </div>
        </div>
        <!-- /.navbar-collapse -->
    </div>
    <!-- /.container -->
</nav>
<script>
    // Drop Bootstarp low-performance Navbar
    // Use customize navbar with high-quality material design animation
    // in high-perf jank-free CSS3 implementation
    var $body   = document.body;
    var $toggle = document.querySelector('.navbar-toggle');
    var $navbar = document.querySelector('#huxblog_navbar');
    var $collapse = document.querySelector('.navbar-collapse');

    $toggle.addEventListener('click', handleMagic)
    function handleMagic(e){
        if ($navbar.className.indexOf('in') > 0) {
        // CLOSE
            $navbar.className = " ";
            // wait until animation end.
            setTimeout(function(){
                // prevent frequently toggle
                if($navbar.className.indexOf('in') < 0) {
                    $collapse.style.height = "0px"
                }
            },400)
        }else{
        // OPEN
            $collapse.style.height = "auto"
            $navbar.className += " in";
        }
    }
</script>


    <!-- Main Content -->
    
<!-- Image to hack wechat -->
<!-- <img src="http://gloomymoon.github.io/img/icon_wechat.png" width="0" height="0"> -->
<!-- <img src="{{ site.baseurl }}/{% if page.header-img %}{{ page.header-img }}{% else %}{{ site.header-img }}{% endif %}" width="0" height="0"> -->

<!-- Post Header -->
<style type="text/css">
    header.intro-header{
        background-image: url('/img/lego-star-wars-war-all-about-pokeman101.jpg')
    }
</style>
<header class="intro-header" >
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <div class="post-heading">
                    <div class="tags">
                        
                          <a class="tag" href="/tags/#Python" title="Python">Python</a>
                        
                          <a class="tag" href="/tags/#Machine Learning" title="Machine Learning">Machine Learning</a>
                        
                          <a class="tag" href="/tags/#scikit-learn" title="scikit-learn">scikit-learn</a>
                        
                    </div>
                    <h1>Python Machine Learning IV</h1>
                    <h2 class="subheading"></h2>
                    <span class="meta">
                        Posted by Gloomymoon on
                        2017-01-25
                    </span>
                </div>
            </div>
        </div>
    </div>
</header>

<!-- Post Content -->
<article>
    <div class="container">
        <div class="row">

    <!-- Post Container -->
            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                post-container">

                <h2 id="4-Building-Good-Training-Sets-–-Data-Preprocessing"><a href="#4-Building-Good-Training-Sets-–-Data-Preprocessing" class="headerlink" title="4 Building Good Training Sets – Data Preprocessing"></a>4 Building Good Training Sets – Data Preprocessing</h2><p>数据的质量和有效信息含量直接决定了机器学习算法能够学得多好。因此在建模前绝对应当对数据进行细查和预处理。本章节将介绍构建模型前必须要具备的数据预处理技术。</p>
<ul>
<li>清除或插补缺失值</li>
<li>将分类数据塑形成模型可用的形式</li>
<li>为模型构建选择相关的特征变量</li>
</ul>
<h3 id="Dealing-with-missing-data"><a href="#Dealing-with-missing-data" class="headerlink" title="Dealing with missing data"></a>Dealing with missing data</h3><p>我们通常将缺失值视为空格或者<code>NaN</code>。不幸的是，很多计算工具无法处理这类缺失值或者会产生无法预测的结果，因此在进一步分析前需要预先处理这类缺失值。讨论这些方法前我们先来创建一个简单的样例数据，这是一个CSV（comma-separated values）文件。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"><span class="keyword">from</span> io <span class="keyword">import</span> StringIO</div><div class="line">csv_data = <span class="string">'''A,B,C,D</span></div><div class="line">    1.0,2.0,3.,4.0</div><div class="line">    5.0,6.0,,8.0</div><div class="line">    0.0,11.0,12.0,'''</div><div class="line">csv_data = unicode(csv_data)</div><div class="line">df = pd.read_csv(StringIO(csv_data))</div><div class="line">df</div></pre></td></tr></table></figure></p>
<p>Output:<br><figure class="highlight lsl"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">A	B	C	D</div><div class="line"><span class="number">0</span>	<span class="number">1</span>	<span class="number">2</span>	<span class="number">3</span>	<span class="number">4</span></div><div class="line"><span class="number">1</span>	<span class="number">5</span>	<span class="number">6</span>	NaN	<span class="number">8</span></div><div class="line"><span class="number">2</span>	<span class="number">0</span>	<span class="number">11</span>	<span class="number">12</span>	NaN</div></pre></td></tr></table></figure></p>
<p>从输出结果看到我们从CSV格式的数据读入到DataFrame对象后，缺失值被替换为<code>NaN</code>。如果使用Python3，则无需使用unicode函数。对于更大的DataFrame，可以使用<code>insnull</code>方法查看每个单元是否含有数值类型的值，然后用<code>sum</code>方法统计缺失的数量。<br><figure class="highlight css"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="selector-tag">df</span><span class="selector-class">.isnull</span>()<span class="selector-class">.sum</span>()</div></pre></td></tr></table></figure></p>
<p>Output:<br><figure class="highlight mathematica"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">A    <span class="number">0</span></div><div class="line">B    <span class="number">0</span></div><div class="line"><span class="keyword">C</span>    <span class="number">1</span></div><div class="line"><span class="keyword">D</span>    <span class="number">1</span></div><div class="line">dtype: int64</div></pre></td></tr></table></figure></p>
<blockquote>
<p>scikit-learn是基于NumPy开发的，有时候用DataFrame处理数据更加便利，因此我们可以通过DataFrame的<code>values</code>方法获取NumPy数组类型的的数据，并将它喂给scikit-learn的算法。</p>
</blockquote>
<p><strong>Eliminating samples or features with missing values</strong><br>最简单的缺失处理方法是扔掉对应的特征（列）或样本（行）。列和行可以方便地通过<code>dropna</code>方法剔除掉。<br><figure class="highlight less"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="selector-tag">df</span><span class="selector-class">.dropna</span>()</div><div class="line"><span class="selector-tag">df</span><span class="selector-class">.dropna</span>(axis=<span class="number">1</span>)</div></pre></td></tr></table></figure></p>
<p>Output:<br><figure class="highlight lsl"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">	A	B	C	D</div><div class="line"><span class="number">0</span>	<span class="number">1</span>	<span class="number">2</span>	<span class="number">3</span>	<span class="number">4</span></div><div class="line"></div><div class="line">	A	B</div><div class="line"><span class="number">0</span>	<span class="number">1</span>	<span class="number">2</span></div><div class="line"><span class="number">1</span>	<span class="number">5</span>	<span class="number">6</span></div><div class="line"><span class="number">2</span>	<span class="number">0</span>	<span class="number">11</span></div></pre></td></tr></table></figure></p>
<p><code>dropna</code>方法还有一些额外的参数可以实现更加灵活的剔除逻辑。<br><figure class="highlight vala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="meta"># 仅剔除所有特征都为NaN的记录</span></div><div class="line">df.dropna(how=<span class="string">'all'</span>)</div><div class="line"></div><div class="line"><span class="meta"># 仅剔除至少有4个特征为NaN的记录</span></div><div class="line">df.dropna(thresh=<span class="number">4</span>)</div><div class="line"></div><div class="line"><span class="meta"># 仅剔除特定列（这里是C）是NaN的记录</span></div><div class="line">df.dropna(subset=[<span class="string">'C'</span>])</div></pre></td></tr></table></figure></p>
<p>剔除缺失数据简单，但是可能会丢失过多的样本或者太多特征变量，损失对分类算法来说有用的信息。</p>
<p><strong>Imputing missing values</strong><br>通过其他样本的数据来对缺失值应用各种插补技术，是另外一种处理方式。一个最常用的方式是使用均值插补，scikit-learn的<code>Imputer</code>类可以方便地实现这类工作。<br><figure class="highlight nix"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">from sklearn.preprocessing <span class="built_in">import</span> Imputer</div><div class="line"><span class="attr">imr</span> = Imputer(<span class="attr">missing_values='NaN',</span> <span class="attr">strategy='mean',</span> <span class="attr">axis=0)</span></div><div class="line"><span class="attr">imr</span> = imr.fit(df)</div><div class="line"><span class="attr">imputed_data</span> = imr.transform(df.values)</div><div class="line">imputed_data</div></pre></td></tr></table></figure></p>
<p>Output:<br><figure class="highlight lsl"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">array([[  <span class="number">1.</span> ,   <span class="number">2.</span> ,   <span class="number">3.</span> ,   <span class="number">4.</span> ],</div><div class="line">       [  <span class="number">5.</span> ,   <span class="number">6.</span> ,   <span class="number">7.5</span>,   <span class="number">8.</span> ],</div><div class="line">       [  <span class="number">0.</span> ,  <span class="number">11.</span> ,  <span class="number">12.</span> ,   <span class="number">6.</span> ]])</div></pre></td></tr></table></figure></p>
<p>该例子中将缺失值<code>NaN</code>替换为每列的平均值，如果将参数<code>axis=0</code>改为<code>axis=1</code>，将替换为行的平均值。<code>strategy</code>参数其他选项还有<code>median</code>和<code>most_frequent</code>，表示中值和最常出现的值。</p>
<p><strong>Understanding the scikit-learn estimator API</strong><br><code>Imputer</code>类在scikit-learn中属于transformer类，主要用来对数据进行各种变形，通常包含两个重要的方法<code>fit</code>和<code>transform</code>。<code>fit</code>方法用来在训练集上学习参数，然后通过<code>transform</code>方法和参数对训练数据集进行转换。被转换的数据集必须与学习的数据集具有相同的特征变量数，下图展现了学习参数并应用于新数据集的转化过程。<br><img src="/img/PythonMachineLearningIV_01.png" alt=""></p>
<h3 id="Handling-categorical-data"><a href="#Handling-categorical-data" class="headerlink" title="Handling categorical data"></a>Handling categorical data</h3><p>至此，我们处理的都是数值类型特征，但在真实数据世界中存在各种分类（categorical）特征数据。分类数据可以分为有序的（ordinal）和无序的（nominal），有序的特征例如T恤衫的尺寸，因为根据定义XL大于L大于M。无序的特征比如T恤衫的颜色，颜色之间的大小排序没有现实意义。<br>同样我们先来创建一个样例数据集。<br><figure class="highlight prolog"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">import pandas as pd</div><div class="line">df = pd.<span class="symbol">DataFrame</span>([</div><div class="line">        [<span class="string">'green'</span>, <span class="string">'M'</span>, <span class="number">10.1</span>, <span class="string">'class1'</span>],</div><div class="line">        [<span class="string">'red'</span>, <span class="string">'L'</span>, <span class="number">13.5</span>, <span class="string">'class2'</span>],</div><div class="line">        [<span class="string">'blue'</span>, <span class="string">'XL'</span>, <span class="number">15.3</span>, <span class="string">'class1'</span>]</div><div class="line">    ])</div><div class="line">df.columns=[<span class="string">'color'</span>, <span class="string">'size'</span>, <span class="string">'price'</span>, <span class="string">'classlabel'</span>]</div><div class="line">df</div></pre></td></tr></table></figure></p>
<p>Output:<br><figure class="highlight lsl"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">	color	size	price	classlabel</div><div class="line"><span class="number">0</span>	green	M	<span class="number">10.1</span>	class1</div><div class="line"><span class="number">1</span>	red	L	<span class="number">13.5</span>	class2</div><div class="line"><span class="number">2</span>	blue	XL	<span class="number">15.3</span>	class1</div></pre></td></tr></table></figure></p>
<p>该数据中包含一个无序分类特征（颜色），一个有序分类特征（尺寸），一个数值特征（价格），和一个分类标识。本书中讨论的分类算法都无视分类标识中的大小和优先关系。</p>
<p><strong>Mapping ordinal features</strong><br>我们必须手工定义有序分类特征到整形的映射逻辑关系，假设我们知道特征之间的区别关系，如：XL=L+1=M+2<br><figure class="highlight prolog"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">size_mapping = &#123;<span class="string">'XL'</span>: <span class="number">3</span>,</div><div class="line">                <span class="string">'L'</span>: <span class="number">2</span>,</div><div class="line">                <span class="string">'M'</span>: <span class="number">1</span>&#125;</div><div class="line">df[<span class="string">'size'</span>] = df[<span class="string">'size'</span>].map(size_mapping)</div><div class="line">df</div></pre></td></tr></table></figure></p>
<p>Output:<br><figure class="highlight lsl"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">	color	size	price	classlabel</div><div class="line"><span class="number">0</span>	green	<span class="number">1</span>	<span class="number">10.1</span>	class1</div><div class="line"><span class="number">1</span>	red	<span class="number">2</span>	<span class="number">13.5</span>	class2</div><div class="line"><span class="number">2</span>	blue	<span class="number">3</span>	<span class="number">15.3</span>	class1</div></pre></td></tr></table></figure></p>
<p>如果需要将整型转会原始的字符串，可以定义逆映射<code>inv_size_mapping = {v:k for k, v in size_mapping.items()}</code>，然后同样适用<code>map</code>方法做一次转换。</p>
<p><strong>Encoding class labels</strong><br>许多机器学习库都要求分类标识必须使用整型数值，我们可以使用类似于对有序分类特征映射的方法对分类标签进行转换，由于分类标识之间没有优先关系，所以具体数值大小无关紧要，我们可以简单从0开始枚举。<br><figure class="highlight ceylon"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy as np</div><div class="line"><span class="keyword">class</span><span class="number">_m</span>apping = &#123;label:idx <span class="keyword">for</span> idx, label <span class="keyword">in</span> enumerate(np.unique(df[<span class="string">'classlabel'</span>]))&#125;</div><div class="line"><span class="keyword">class</span><span class="number">_m</span>apping</div><div class="line">df[<span class="string">'classlabel'</span>] = df[<span class="string">'classlabel'</span>].map(<span class="keyword">class</span><span class="number">_m</span>apping)</div><div class="line">df</div></pre></td></tr></table></figure></p>
<p>Output:<br><figure class="highlight processing"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">&#123;<span class="string">'class1'</span>: <span class="number">0</span>, <span class="string">'class2'</span>: <span class="number">1</span>&#125;</div><div class="line"></div><div class="line">	<span class="built_in">color</span>	<span class="built_in">size</span>	price	classlabel</div><div class="line"><span class="number">0</span>	<span class="built_in">green</span>	<span class="number">1</span>	<span class="number">10.1</span>	<span class="number">0</span></div><div class="line"><span class="number">1</span>	<span class="built_in">red</span>	<span class="number">2</span>	<span class="number">13.5</span>	<span class="number">1</span></div><div class="line"><span class="number">2</span>	<span class="built_in">blue</span>	<span class="number">3</span>	<span class="number">15.3</span>	<span class="number">0</span></div></pre></td></tr></table></figure></p>
<p>scikit-learn直接实现了一个更方便的LabelEncoder类可以直接实现上述功能。<br><figure class="highlight ceylon"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">inv<span class="number">_</span><span class="keyword">class</span><span class="number">_m</span>apping = &#123;v: k <span class="keyword">for</span> k, v <span class="keyword">in</span> <span class="keyword">class</span><span class="number">_m</span>apping.items()&#125;</div><div class="line">df[<span class="string">'classlabel'</span>] = df[<span class="string">'classlabel'</span>].map(inv<span class="number">_</span><span class="keyword">class</span><span class="number">_m</span>apping)</div><div class="line">from sklearn.preprocessing <span class="keyword">import</span> LabelEncoder</div><div class="line"><span class="keyword">class</span><span class="number">_</span>le = LabelEncoder()</div><div class="line">y = <span class="keyword">class</span><span class="number">_</span>le.fit<span class="number">_</span>transform(df[<span class="string">'classlabel'</span>].values)</div><div class="line">y</div></pre></td></tr></table></figure></p>
<p>Output:<br><figure class="highlight lsl"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">array([<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>])</div></pre></td></tr></table></figure></p>
<p><code>fit_transform</code>是调用<code>fit</code>和<code>transform</code>的快捷方式，另外还可以直接使用<code>inverse_transform</code>进行逆向转换。<br><figure class="highlight ceylon"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">class</span><span class="number">_</span>le.inverse<span class="number">_</span>transform(y)</div></pre></td></tr></table></figure></p>
<p><strong>Performing ont-hot encoding on nominal features</strong><br>可以使用同样的技术对无序分类特征转换成整型，例如：blue -&gt; 0, green -&gt; 1, red -&gt; 2，机器学习算法会假设绿色大于蓝色，红色大于绿色，尽管这个假设并不正确，但是仍然可以得出一些有用的结果（当然不是最优的）。<br>一个常用的解决是独热编码（one-hot encoding），通过给变量的每一个可能取值都创建一个独立的哑变量（dummy feature）。本例中，可以将颜色特征转换成三个新的变量：blue, green, red，每个变量都通过二元标识来表示对应的颜色取值情况，比如蓝色样本的变量取值为：blue=1, green=0, red=0。scikit-learn.preprocessing模块中的OneHotEncoder实现了该类转换功能.<br><figure class="highlight prolog"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="symbol">X</span> = df[[<span class="string">'color'</span>, <span class="string">'size'</span>, <span class="string">'price'</span>]].values</div><div class="line">color_le = <span class="symbol">LabelEncoder</span>()</div><div class="line"><span class="symbol">X</span>[:, <span class="number">0</span>] = color_le.fit_transform(<span class="symbol">X</span>[:, <span class="number">0</span>])</div><div class="line">from sklearn.preprocessing import <span class="symbol">OneHotEncoder</span></div><div class="line">ohe = <span class="symbol">OneHotEncoder</span>(categorical_features=[<span class="number">0</span>])</div><div class="line">ohe.fit_transform(<span class="symbol">X</span>).toarray()</div></pre></td></tr></table></figure></p>
<p>Output:<br><figure class="highlight lsl"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">array([[  <span class="number">0.</span> ,   <span class="number">1.</span> ,   <span class="number">0.</span> ,   <span class="number">1.</span> ,  <span class="number">10.1</span>],</div><div class="line">       [  <span class="number">0.</span> ,   <span class="number">0.</span> ,   <span class="number">1.</span> ,   <span class="number">2.</span> ,  <span class="number">13.5</span>],</div><div class="line">       [  <span class="number">1.</span> ,   <span class="number">0.</span> ,   <span class="number">0.</span> ,   <span class="number">3.</span> ,  <span class="number">15.3</span>]])</div></pre></td></tr></table></figure></p>
<p>DataFrame甚至有一个更加简单的<code>get_dummies</code>方法可以直接将字符类型变量转换成哑变量。<br><figure class="highlight lua"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">pd.get_dummies(df<span class="string">[['price', 'color', 'size']]</span>)</div></pre></td></tr></table></figure></p>
<p>Output:<br><figure class="highlight lsl"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">	price	size	color_blue	color_green	color_red</div><div class="line"><span class="number">0</span>	<span class="number">10.1</span>	<span class="number">1</span>	<span class="number">0.0</span>	<span class="number">1.0</span>	<span class="number">0.0</span></div><div class="line"><span class="number">1</span>	<span class="number">13.5</span>	<span class="number">2</span>	<span class="number">0.0</span>	<span class="number">0.0</span>	<span class="number">1.0</span></div><div class="line"><span class="number">2</span>	<span class="number">15.3</span>	<span class="number">3</span>	<span class="number">1.0</span>	<span class="number">0.0</span>	<span class="number">0.0</span></div></pre></td></tr></table></figure></p>
<h3 id="Partioning-a-dataset-in-training-and-test-sets"><a href="#Partioning-a-dataset-in-training-and-test-sets" class="headerlink" title="Partioning a dataset in training and test sets"></a>Partioning a dataset in training and test sets</h3><p>第一章和第三章都介绍过将建模数据切分为训练集和测试集的概念，本节我们将引入一个新的数据集，通过数据预处理学习几种特征选择的降维技术。</p>
<p>新数据是由<a href="https://archive.ics.uci.edu/ml/datasets/Wine" target="_blank" rel="external">UCI</a>提供的葡萄酒数据，其包含了178个酒品样本和13个描述化学属性的特征变量，通过pandas我们可以直接从互联网都如该数据。<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">df_wine = pd.read_csv(<span class="string">'https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data'</span>, header=None)</div><div class="line">df_wine<span class="selector-class">.columns</span> = [<span class="string">'Class label'</span>, <span class="string">'Alcohol'</span>, <span class="string">'Malic acid'</span>, <span class="string">'Ash'</span>,</div><div class="line">                   <span class="string">'Alcalinity of ash'</span>, <span class="string">'Magnesium'</span>, <span class="string">'Total phenols'</span>, <span class="string">'Flavanoids'</span>,</div><div class="line">                   <span class="string">'Nonflavanoid phenols'</span>, <span class="string">'Proanthocyanins'</span>, <span class="string">'Color intensity'</span>, <span class="string">'Hue'</span>,</div><div class="line">                   <span class="string">'OD280/OD315 of diluted wines'</span>, <span class="string">'Proline'</span>]</div><div class="line"><span class="function"><span class="title">print</span><span class="params">(<span class="string">'Class labels'</span>, np.unique(df_wine[<span class="string">'Class label'</span>])</span></span>)</div><div class="line">df_wine.head()</div></pre></td></tr></table></figure></p>
<p>Output:</p>
<figure class="highlight clojure"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">(<span class="name">'Class</span> labels', array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], dtype=int64))</div></pre></td></tr></table></figure>
<p>样本包含3个分类，1、2和3，对应意大利不同区域种植的三种不同的葡萄。<br>通过cross_validation模块的<code>train_test_split</code>方法可以将数据随机分到测试集和训练集。</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">from sklearn<span class="selector-class">.cross_validation</span> import train_test_split</div><div class="line">X, y = df_wine<span class="selector-class">.iloc</span>[:, <span class="number">1</span>:]<span class="selector-class">.values</span>, df_wine<span class="selector-class">.iloc</span>[:, <span class="number">0</span>]<span class="selector-class">.values</span></div><div class="line">X_train, X_test, y_train, y_test = \</div><div class="line">    train_test_split(X, y, test_size=<span class="number">0.3</span>, random_state=<span class="number">0</span>)</div></pre></td></tr></table></figure>
<h3 id="Bringing-features-onto-the-same-scale"><a href="#Bringing-features-onto-the-same-scale" class="headerlink" title="Bringing features onto the same scale"></a>Bringing features onto the same scale</h3><p>特征归一化（feature scaling）是预处理前容易被忽略的重要步骤，除了决策树和随机森林等少数几类算法，将特征变量映射到统一尺度下有助于大多数机器学习算法和调优，通常来说主要有两种方法：归一化（normalization）和标准化（standardization）。归一化指将特征重新映射到[0, 1]的范围，这是一种最大最小归一化的特殊情况（min-max scaling）。每个样本的特征值xi可以按照下述公式得到新值：x’ = (x - min(x))/(max(x)-min(x))。scikit-learn中可以直接使用MinMaxScaler实现。</p>
<figure class="highlight nix"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">from sklearn.preprocessing <span class="built_in">import</span> MinMaxScaler</div><div class="line"><span class="attr">mms</span> = MinMaxScaler()</div><div class="line"><span class="attr">X_train_norm</span> = mms.fit_transform(X_train)</div><div class="line"><span class="attr">X_test_norm</span> = mms.transform(X_test)</div></pre></td></tr></table></figure>
<p>实际中标准化比归一化更加常用，因为许多线性模型（如逻辑回归和SBM）的权重初始值会设置为0或者接近0的小随机数，变量经过标准化后形成正态分布，均值为0，标准差为1，标准化还能够保留离群值（outliers）信息，但又不会影响算法。标准化常用的方法如下：<br><img src="/img/PythonMachineLearningIV_02.png" alt=""></p>
<p>其中µ 是样本均值，σ 是标准差。</p>
<p>下表展示了同一个变量经过归一化和标准化之后的结果：</p>
<figure class="highlight css"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="selector-tag">input</span> <span class="selector-tag">standardized</span> <span class="selector-tag">normalized</span></div><div class="line">0<span class="selector-class">.0</span> <span class="selector-tag">-1</span><span class="selector-class">.336306</span> 0<span class="selector-class">.0</span></div><div class="line">1<span class="selector-class">.0</span> <span class="selector-tag">-0</span><span class="selector-class">.801784</span> 0<span class="selector-class">.2</span></div><div class="line">2<span class="selector-class">.0</span> <span class="selector-tag">-0</span><span class="selector-class">.267261</span> 0<span class="selector-class">.4</span></div><div class="line">3<span class="selector-class">.0</span> 0<span class="selector-class">.267261</span> 0<span class="selector-class">.6</span></div><div class="line">4<span class="selector-class">.0</span> 0<span class="selector-class">.801784</span> 0<span class="selector-class">.8</span></div><div class="line">5<span class="selector-class">.0</span> 1<span class="selector-class">.336306</span> 1<span class="selector-class">.0</span></div></pre></td></tr></table></figure>
<p>scikit-learn同样实现了标准化的类：</p>
<figure class="highlight nix"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">from sklearn.preprocessing <span class="built_in">import</span> StandardScaler</div><div class="line"><span class="attr">stdsc</span> = StandardScaler()</div><div class="line"><span class="attr">X_train_std</span> = stdsc.fit_transform(X_train)</div><div class="line"><span class="attr">X_test_std</span> = stdsc.transform(X_test)</div><div class="line">X_test_std</div></pre></td></tr></table></figure>
<p>注意StandardScaler只训练一次，然后用来后续的所有测试和验证数据集。</p>
<blockquote>
<p>更加细节内容可以参照：<a href="http://www.zhaokv.com/2016/01/normalization-and-standardization.html" target="_blank" rel="external">http://www.zhaokv.com/2016/01/normalization-and-standardization.html</a></p>
</blockquote>
<h3 id="Selecting-meaningful-features"><a href="#Selecting-meaningful-features" class="headerlink" title="Selecting meaningful features"></a>Selecting meaningful features</h3><p>如果一个模型在训练集上的表现明显优于测试集，则意味着过拟合，也称为高方差（variance），通常是模型过于复杂造成。解决方案如下：</p>
<ul>
<li><p>收集更多训练数据</p>
</li>
<li><p>进入针对复杂程度的惩罚系数，例如之前介绍的正则化方法</p>
</li>
<li><p>选择一个参数更少的简单模型</p>
</li>
<li><p>对建模数据进行降维</p>
<p>​</p>
</li>
</ul>
<p>收集更多数据通常受到实际限制，本节将介绍正则化和变量选择降维技术来降低过拟合。</p>
<p><strong>Sparse solutions with L1 regularization</strong></p>
<p>第三章中介绍L2 regularization是一种降低模型复杂程度的方法，L1 regularization则是另一种。对于scikit-learn中支持L1 regularization的模型，可以简单的添加<code>penalty</code>参数并设置为`’l1’。</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">from sklearn<span class="selector-class">.linear_model</span> import LogisticRegression</div><div class="line">lr = LogisticRegression(penalty=<span class="string">'l1'</span>, C=<span class="number">0.1</span>)</div><div class="line">lr.fit(X_train_std, y_train)</div><div class="line"><span class="function"><span class="title">print</span><span class="params">(<span class="string">'Training accuracy:'</span>, lr.score(X_train_std, y_train)</span></span>)</div><div class="line"><span class="function"><span class="title">print</span><span class="params">(<span class="string">'Test accuracy:'</span>, lr.score(X_test_std, y_test)</span></span>)</div></pre></td></tr></table></figure>
<p>Output:</p>
<figure class="highlight subunit"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">Training accuracy: 0.983870967742</div><div class="line"><span class="keyword">Test </span>accuracy: 0.981481481481</div></pre></td></tr></table></figure>
<p>结果显示模型在训练集上和测试机上都没有出现过拟合。当我们查看<code>lr.intercept_</code>属性，可以看到返回一个有三个值的数组：</p>
<figure class="highlight dns"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">array([-<span class="number">0.38381622</span>, -<span class="number">0.15806481</span>, -<span class="number">0.70048192</span>])</div></pre></td></tr></table></figure>
<p>由于我们用LogisticRegression在一个多分类数据上建模，算法默认会使用One-vs-Rest方法，第一个参数对应的是分类1对分类2和分类3，第二个参数是分类2对分类1和分类3，第三个参数是分类3对分类1和分类2。<code>lr.coef_</code>获取的权重数组有三条记录，每条对应一个分类的参数权重。这里每条记录都有13个权重参数对应了13个特征变量。</p>
<figure class="highlight lsl"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">array([[ <span class="number">0.28007103</span>,  <span class="number">0.</span>        ,  <span class="number">0.</span>        , <span class="number">-0.02793263</span>,  <span class="number">0.</span>        ,</div><div class="line">           <span class="number">0.</span>        ,  <span class="number">0.70994966</span>,  <span class="number">0.</span>        ,  <span class="number">0.</span>        ,  <span class="number">0.</span>        ,</div><div class="line">           <span class="number">0.</span>        ,  <span class="number">0.</span>        ,  <span class="number">1.23675898</span>],</div><div class="line">         [<span class="number">-0.64388996</span>, <span class="number">-0.06884945</span>, <span class="number">-0.05719169</span>,  <span class="number">0.</span>        ,  <span class="number">0.</span>        ,</div><div class="line">           <span class="number">0.</span>        ,  <span class="number">0.</span>        ,  <span class="number">0.</span>        ,  <span class="number">0.</span>        , <span class="number">-0.92692532</span>,</div><div class="line">           <span class="number">0.06003993</span>,  <span class="number">0.</span>        , <span class="number">-0.37105</span>   ],</div><div class="line">         [ <span class="number">0.</span>        ,  <span class="number">0.06148265</span>,  <span class="number">0.</span>        ,  <span class="number">0.</span>        ,  <span class="number">0.</span>        ,</div><div class="line">           <span class="number">0.</span>        , <span class="number">-0.63570527</span>,  <span class="number">0.</span>        ,  <span class="number">0.</span>        ,  <span class="number">0.49779924</span>,</div><div class="line">          <span class="number">-0.3583329</span> , <span class="number">-0.57169567</span>,  <span class="number">0.</span>        ]])</div></pre></td></tr></table></figure>
<p>可以看到参数矩阵是稀疏的，意味经过L1正则化完成了特征筛选，使得训练的模型可以不受数据中不相关特征的影响。最后，通过调整正则化力度，观察不同变量的权重系数变化情况。</p>
<figure class="highlight sqf"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div></pre></td><td class="code"><pre><div class="line">import matplotlib.pyplot as plt</div><div class="line">fig = plt.figure()</div><div class="line">ax = plt.subplot(<span class="number">111</span>)</div><div class="line">colors = [<span class="string">'blue'</span>, <span class="string">'green'</span>, <span class="string">'red'</span>, <span class="string">'cyan'</span>, <span class="string">'magenta'</span>, <span class="string">'yellow'</span>, <span class="string">'black'</span>,</div><div class="line">          <span class="string">'pink'</span>, <span class="string">'lightgreen'</span>, <span class="string">'lightblue'</span>, <span class="string">'gray'</span>, <span class="string">'indigo'</span>, <span class="string">'orange'</span>]</div><div class="line">weights, <span class="built_in">params</span> = [], []</div><div class="line"><span class="keyword">for</span> c <span class="built_in">in</span> np.arange(-<span class="number">4</span>, <span class="number">6</span>):</div><div class="line">    lr = LogisticRegression(penalty=<span class="string">'l1'</span>, C=<span class="number">10</span>**c, random_state=<span class="number">0</span>)</div><div class="line">    lr.fit(X_train_std, y_train)</div><div class="line">    weights.<span class="built_in">append</span>(lr.coef_[<span class="number">1</span>])</div><div class="line">    <span class="built_in">params</span>.<span class="built_in">append</span>(<span class="number">10</span>**c)import matplotlib.pyplot as plt</div><div class="line">fig = plt.figure()</div><div class="line">ax = plt.subplot(<span class="number">111</span>)</div><div class="line">colors = [<span class="string">'blue'</span>, <span class="string">'green'</span>, <span class="string">'red'</span>, <span class="string">'cyan'</span>, <span class="string">'magenta'</span>, <span class="string">'yellow'</span>, <span class="string">'black'</span>,</div><div class="line">          <span class="string">'pink'</span>, <span class="string">'lightgreen'</span>, <span class="string">'lightblue'</span>, <span class="string">'gray'</span>, <span class="string">'indigo'</span>, <span class="string">'orange'</span>]</div><div class="line">weights, <span class="built_in">params</span> = [], []</div><div class="line"><span class="keyword">for</span> c <span class="built_in">in</span> np.arange(-<span class="number">4</span>, <span class="number">6</span>):</div><div class="line">    lr = LogisticRegression(penalty=<span class="string">'l1'</span>, C=<span class="number">10</span>**c, random_state=<span class="number">0</span>)</div><div class="line">    lr.fit(X_train_std, y_train)</div><div class="line">    weights.<span class="built_in">append</span>(lr.coef_[<span class="number">1</span>])</div><div class="line">    <span class="built_in">params</span>.<span class="built_in">append</span>(<span class="number">10</span>**c)</div><div class="line">weights = np.<span class="built_in">array</span>(weights)</div><div class="line"><span class="keyword">for</span> column, color <span class="built_in">in</span> zip(range(weights.shape[<span class="number">1</span>]), colors):</div><div class="line">    plt.plot(<span class="built_in">params</span>, weights[:, column], label=df_wine.columns[column+<span class="number">1</span>], color=color)</div><div class="line">plt.axhline(<span class="number">0</span>, color=<span class="string">'black'</span>, linestyle=<span class="string">'--'</span>, linewidth=<span class="number">3</span>)</div><div class="line">plt.xlim([<span class="number">10</span>**(-<span class="number">5</span>), <span class="number">10</span>**<span class="number">5</span>])</div><div class="line">plt.ylabel(<span class="string">'weight coefficient'</span>)</div><div class="line">plt.xlabel(<span class="string">'C'</span>)</div><div class="line">plt.xscale(<span class="string">'log'</span>)</div><div class="line">plt.legend(loc=<span class="string">'upper left'</span>)</div><div class="line">ax.legend(loc=<span class="string">'upper center'</span>, bbox_to_anchor=(<span class="number">1.38</span>, <span class="number">1.03</span>), ncol=<span class="number">1</span>, fancybox=<span class="literal">True</span>)</div><div class="line">plt.show()</div><div class="line">weights = np.<span class="built_in">array</span>(weights)</div><div class="line"><span class="keyword">for</span> column, color <span class="built_in">in</span> zip(range(weights.shape[<span class="number">1</span>]), colors):</div><div class="line">    plt.plot(<span class="built_in">params</span>, weights[:, column], label=df_wine.columns[column+<span class="number">1</span>], color=color)</div><div class="line">plt.axhline(<span class="number">0</span>, color=<span class="string">'black'</span>, linestyle=<span class="string">'--'</span>, linewidth=<span class="number">3</span>)</div><div class="line">plt.xlim([<span class="number">10</span>**(-<span class="number">5</span>), <span class="number">10</span>**<span class="number">5</span>])</div><div class="line">plt.ylabel(<span class="string">'weight coefficient'</span>)</div><div class="line">plt.xlabel(<span class="string">'C'</span>)</div><div class="line">plt.xscale(<span class="string">'log'</span>)</div><div class="line">plt.legend(loc=<span class="string">'upper left'</span>)</div><div class="line">ax.legend(loc=<span class="string">'upper center'</span>, bbox_to_anchor=(<span class="number">1.38</span>, <span class="number">1.03</span>), ncol=<span class="number">1</span>, fancybox=<span class="literal">True</span>)</div><div class="line">plt.show()</div></pre></td></tr></table></figure>
<p><img src="/img/PythonMachineLearningIV_03.png" alt=""></p>
<p>上图展示了L1正则化对所有参数的作用，当正则化参数很强（C&lt;0.1）时，所有特征的权重都会趋于0。</p>
<p><strong>Sequential feature selection algorithms</strong></p>
<p>另一个降低模型复杂度避免过拟合的方法是通过特征筛选进行降维（dimensionality reduction），这对于非正则化（unregularized）模型尤为有用。降维主要有两类技术：特征选择（feature selection）和特征抽取（feature extraction）。前者是从原始变量中选择部分特征，而后者是基于原变量构建一个特征子空间。本章我们将探索特征筛选算法，下章节将介绍特征抽取技术。</p>
<p>序列特征选择（Sequential feature selection）是一类贪婪搜索算法，用于将初始d维特征子集压缩到k维特征子集上。算法的原理是自动选择一个特征子集，通过删除不相关的特征或者噪音数据使得模型能够提升计算效率或降低泛化误差（generalization error）。一个经典的序列特征选择算法是序列后向选择（Sequential Backward Selection，SBS）。SBS从特征全集开始，每次从中剔除一个特征，直到特征数量达到希望值。为了判断每次剔除哪个变量，需要定义一个评价函数J，J可以简单定义为剔除特定变量前后之间的性能差异，这样只要保证每次剔除的变量的评价函数J最小即可。SBS算法没有在scikit-learn中实现，但是因为非常简单，我们可以自行完成：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn.base <span class="keyword">import</span> clone</div><div class="line"><span class="keyword">from</span> itertools <span class="keyword">import</span> combinations</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">from</span> sklearn.cross_validation <span class="keyword">import</span> train_test_split</div><div class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">SBS</span><span class="params">()</span>:</span></div><div class="line">	<span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, estimator, k_features,</span></span></div><div class="line">    	scoring=accuracy_score,</div><div class="line">    	test_size=<span class="number">0.25</span>, random_state=<span class="number">1</span>):</div><div class="line">		self.scoring = scoring</div><div class="line">		self.estimator = clone(estimator)</div><div class="line">		self.k_features = k_features</div><div class="line">		self.test_size = test_size</div><div class="line">		self.random_state = random_state</div><div class="line"></div><div class="line">	<span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, X, y)</span>:</span></div><div class="line">		X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=self.test_size, random_state=self.random_state)</div><div class="line">		dim = X_train.shape[<span class="number">1</span>]</div><div class="line">		self.indices_ = tuple(range(dim))</div><div class="line">		self.subsets_ = [self.indices_]</div><div class="line">		score = self._calc_score(X_train, y_train, X_test, y_test, self.indices_)</div><div class="line">		self.scores_ = [score]</div><div class="line"></div><div class="line">		<span class="keyword">while</span> dim &gt; self.k_features:</div><div class="line">			scores = []</div><div class="line">			subsets = []</div><div class="line"></div><div class="line">			<span class="keyword">for</span> p <span class="keyword">in</span> combinations(self.indices_, r=dim<span class="number">-1</span>):</div><div class="line">				score = self._calc_score(X_train, y_train, X_test, y_test, p)</div><div class="line">				scores.append(score)</div><div class="line">				subsets.append(p)</div><div class="line"></div><div class="line">			best = np.argmax(scores)</div><div class="line">			self.indices_ = subsets[best]</div><div class="line">			self.subsets_.append(self.indices_)</div><div class="line">			dim -= <span class="number">1</span></div><div class="line"></div><div class="line">			self.scores_.append(scores[best])</div><div class="line">		self.k_score_ = self.scores_[<span class="number">-1</span>]</div><div class="line"></div><div class="line">		<span class="keyword">return</span> self</div><div class="line"></div><div class="line">	<span class="function"><span class="keyword">def</span> <span class="title">transform</span><span class="params">(self, X)</span>:</span></div><div class="line">		<span class="keyword">return</span> X[:, self.indices_]</div><div class="line"></div><div class="line">	<span class="function"><span class="keyword">def</span> <span class="title">_calc_score</span><span class="params">(self, X_train, y_train, X_test, y_test, indices)</span>:</span></div><div class="line">		self.estimator.fit(X_train[:, indices], y_train)</div><div class="line">		y_pred = self.estimator.predict(X_test[:, indices])</div><div class="line">		score = self.scoring(y_test, y_pred)</div><div class="line">		<span class="keyword">return</span> score</div></pre></td></tr></table></figure>
<p>我们实现的方法中，<code>k_features</code>参数定义了需要的特征数量，<code>scoreing</code>参数默认使用scikit-learn的<code>accuracy_score</code>来评估特征子集上的模型分类效果。在<code>while</code>循环中的<code>fit</code>方法，<code>itertools.combination</code>方法不断精简生成新的特征子集并评价模型表现，知道特征数量满足我们的制定要求。每一次循环中表现最好的特征子集的准确度存放于<code>self.scores_</code>列表中，最终特征变量的下标会存储在<code>self.indices_</code>中，可以方便的用<code>transform</code>方法生成选中特征的数据集。注意在<code>fit</code>方法中，我们简单的将不在最佳表现特征子集中的列去除，而没有显示计算各特征组合的差异。</p>
<p>现在我们可以将SBS算法应用于KNN分类算法中实践一下：</p>
<figure class="highlight nix"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">from sklearn.neighbors <span class="built_in">import</span> KNeighborsClassifier</div><div class="line"><span class="built_in">import</span> matplotlib.pyplot as plt</div><div class="line"><span class="attr">knn</span> = KNeighborsClassifier(<span class="attr">n_neighbors=2)</span></div><div class="line"><span class="attr">sbs</span> = SBS(knn, <span class="attr">k_features=1)</span></div><div class="line">sbs.fit(X_train_std, y_train)</div></pre></td></tr></table></figure>
<p>我们在SBS实现中已经在<code>fit</code>方法中将输入数据集拆分为训练和测试集，所以可以直接输入<code>X_train</code>训练集。这个步骤必不可少，可以避免我们原始的测试数据成为训练数据的一部分。</p>
<figure class="highlight maxima"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">k_feat = [len(k) <span class="keyword">for</span> k <span class="keyword">in</span> sbs.subsets_]</div><div class="line">plt.plot(k_feat, sbs.scores_, marker='o')</div><div class="line">plt.ylim([<span class="number">0.7</span>, <span class="number">1.1</span>])</div><div class="line">plt.<span class="built_in">ylabel</span>('Accuracy')</div><div class="line">plt.<span class="built_in">xlabel</span>('Number of <span class="built_in">features</span>')</div><div class="line">plt.<span class="built_in">grid</span>()</div><div class="line">plt.<span class="built_in">show</span>()</div></pre></td></tr></table></figure>
<p><img src="/img/PythonMachineLearningIV_04.png" alt=""></p>
<p>从上图结果可以看到，当特征数量下降时，KNN分类器的准确度获得提升。为了满足我们的好奇心，可以查看准确度达到100%时的5个特征是什么，通过获取<code>sbs.subsets_</code>的第9个（也就是特征数为5时）的特征变量名：</p>
<figure class="highlight markdown"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">k5 = list(sbs.subsets_[8])</div><div class="line">print(df_wine.columns[<span class="string">1:</span>][<span class="symbol">k5</span>])</div></pre></td></tr></table></figure>
<p>Output:</p>
<figure class="highlight sml"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="type">Index</span>([<span class="symbol">'Alcohol'</span>, <span class="symbol">'Malic</span> acid', <span class="symbol">'Alcalinity</span> <span class="keyword">of</span> ash', <span class="symbol">'Hue'</span>, <span class="symbol">'Proline'</span>], dtype=<span class="symbol">'object'</span>)</div></pre></td></tr></table></figure>
<p>下一步，验证一下KNN分类器在原始测试集上的表现性能：</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">knn.fit(X_train_std, y_train)</div><div class="line"><span class="function"><span class="title">print</span><span class="params">(<span class="string">'Training accuracy: '</span>, knn.score(X_train_std, y_train)</span></span>)</div><div class="line"><span class="function"><span class="title">print</span><span class="params">(<span class="string">"Test accuracy: "</span>, knn.score(X_test_std, y_test)</span></span>)</div></pre></td></tr></table></figure>
<p>Output:</p>
<figure class="highlight subunit"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">Training accuracy:  0.983870967742</div><div class="line"></div><div class="line"><span class="keyword">Test </span>accuracy:  0.944444444444</div></pre></td></tr></table></figure>
<p>首先我们使用完整特征集训练的模型，在训练集上精准度约为98.4%，在测试集上准确度誉为94.4%，显示模型有一定程度的过拟合。现在我们使用选择的5个特征来重新训练：</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">knn.fit(X_train_std[:, k5], y_train)</div><div class="line"><span class="function"><span class="title">print</span><span class="params">(<span class="string">'Training accuracy: '</span>, knn.score(X_train_std[:, k5], y_train)</span></span>)</div><div class="line"><span class="function"><span class="title">print</span><span class="params">(<span class="string">'Test accuracy: '</span>, knn.score(X_test_std[:, k5], y_test)</span></span>)</div></pre></td></tr></table></figure>
<p>Output:</p>
<figure class="highlight subunit"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">Training accuracy:  0.959677419355</div><div class="line"></div><div class="line"><span class="keyword">Test </span>accuracy:  0.962962962963</div></pre></td></tr></table></figure>
<p>使用比原来少一半的特征，KNN模型在测试数据集上的精准度提升了2个百分点，并且与训练集上的精准度差异明显小，显著降低模型的过拟合度。</p>
<blockquote>
<p>Feature selection algorithms in scikit-learn</p>
<p>scikit-learn中有很多特征选择算法，可以参见<a href="http://scikit
learn.org/stable/modules/feature_selection.html" target="_blank" rel="external"></a>，也可以参见<a href="http://www.cnblogs.com/heaad/archive/2011/01/02/1924088.html" target="_blank" rel="external"></a></p>
</blockquote>
<h3 id="Assessing-feature-importance-with-random-forests"><a href="#Assessing-feature-importance-with-random-forests" class="headerlink" title="Assessing feature importance with random forests"></a>Assessing feature importance with random forests</h3><p>除了用SBS算法和逻辑回归，还可以利用随机森林来选择相关特征。通过随机森林，在无需关注数据是否线性可分的情况下，直接通过所有决策树计算出的平均杂质度减少情况来评估特征的重要程度。scikit-learn中的随机森林算法已经手机了特征重要度，我们可以在训练后方便地通过<code>feature_importances_</code>属性来访问。下面的代码中，我们现在Wine数据上训练10000棵树的随机森林，并对13个特征的重要性排名。基于树的模型无需标准化和规范化。</p>
<figure class="highlight nix"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">from sklearn.ensemble <span class="built_in">import</span> RandomForestClassifier</div><div class="line"><span class="attr">feat_labels</span> = df_wine.columns[<span class="number">1</span>:]</div><div class="line"><span class="attr">forest</span> = RandomForestClassifier(<span class="attr">n_estimators=10000,</span> <span class="attr">random_state=0,</span> <span class="attr">n_jobs=-1)</span></div><div class="line">forest.fit(X_train, y_train)</div><div class="line"><span class="attr">importances</span> = forest.feature_importances_</div><div class="line"><span class="attr">indices</span> = np.argsort(importances)[::-<span class="number">1</span>]</div><div class="line">for f <span class="keyword">in</span> range(X_train.shape[<span class="number">1</span>]):</div><div class="line">    print(<span class="string">"%2d) %-*s %f"</span> % (f + <span class="number">1</span>, <span class="number">30</span>, feat_labels[f], importances[indices[f]]))</div></pre></td></tr></table></figure>
<p>Output:</p>
<figure class="highlight lsl"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"> <span class="number">1</span>) Alcohol                        <span class="number">0.182483</span></div><div class="line"> <span class="number">2</span>) Malic acid                     <span class="number">0.158610</span></div><div class="line"> <span class="number">3</span>) Ash                            <span class="number">0.150948</span></div><div class="line"> <span class="number">4</span>) Alcalinity of ash              <span class="number">0.131987</span></div><div class="line"> <span class="number">5</span>) Magnesium                      <span class="number">0.106589</span></div><div class="line"> <span class="number">6</span>) Total phenols                  <span class="number">0.078243</span></div><div class="line"> <span class="number">7</span>) Flavanoids                     <span class="number">0.060718</span></div><div class="line"> <span class="number">8</span>) Nonflavanoid phenols           <span class="number">0.032033</span></div><div class="line"> <span class="number">9</span>) Proanthocyanins                <span class="number">0.025400</span></div><div class="line"><span class="number">10</span>) Color intensity                <span class="number">0.022351</span></div><div class="line"><span class="number">11</span>) Hue                            <span class="number">0.022078</span></div><div class="line"><span class="number">12</span>) OD280/OD315 of diluted wines   <span class="number">0.014645</span></div><div class="line"><span class="number">13</span>) Proline                        <span class="number">0.013916</span></div></pre></td></tr></table></figure>
<p>所有特征的重要度已经经过规范化，它们的总和等于1.0。通过10000棵决策树训练的结论是alcohol是区分酒对重要的特征变量，且排名靠前的3个特征也在之前SBS算法选择的5个特征中。不过就可解释性而言，随机森林有个问题需要注意，如果两个和多个特征高度相关，一个特征会获得很高的排名而其它相关的特征会被忽视。如果我们仅仅关心模型的预测能力而不用解释变量的重要性则不必过分关注这个问题。</p>
<p>最后，scikit-learn中的随机森林分类器同样实现了<code>transform</code>方法，可以基于用户指定的阈值选择特征变量，例如我们可以设定阈值为0.15，将Wine数据集的变量限定在最重要的三个。</p>
<figure class="highlight nix"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="attr">X_selected</span> = forest.transform(X_train, <span class="attr">threshold=0.15)</span></div><div class="line">X_selected.shape</div></pre></td></tr></table></figure>
<p>Output:</p>
<figure class="highlight lsl"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">(<span class="number">124</span>, <span class="number">3</span>)</div></pre></td></tr></table></figure>


                <hr>

                

                <ul class="pager">
                    
                        <li class="previous">
                            <a href="/2017/01/25/Python-Machine-Learning-V/" data-toggle="tooltip" data-placement="top" title="Python Machine Learning IV">&larr; Previous Post</a>
                        </li>
                    
                    
                        <li class="next">
                            <a href="/2017/01/21/Python-Machine-Learning-III/" data-toggle="tooltip" data-placement="top" title="Python Machine Learning III">Next Post &rarr;</a>
                        </li>
                    
                </ul>

                

                

            </div>
    <!-- Side Catalog Container -->
        

    <!-- Sidebar Container -->

            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                sidebar-container">

                <!-- Featured Tags -->
                
                <section>
                    <!-- no hr -->
                    <h5><a href="/tags/">FEATURED TAGS</a></h5>
                    <div class="tags">
                       
                          <a class="tag" href="/tags/#Python" title="Python">Python</a>
                        
                          <a class="tag" href="/tags/#Machine Learning" title="Machine Learning">Machine Learning</a>
                        
                          <a class="tag" href="/tags/#scikit-learn" title="scikit-learn">scikit-learn</a>
                        
                    </div>
                </section>
                

                <!-- Friends Blog -->
                
            </div>

        </div>
    </div>
</article>







<!-- async load function -->
<script>
    function async(u, c) {
      var d = document, t = 'script',
          o = d.createElement(t),
          s = d.getElementsByTagName(t)[0];
      o.src = u;
      if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
      s.parentNode.insertBefore(o, s);
    }
</script>
<!-- anchor-js, Doc:http://bryanbraun.github.io/anchorjs/ -->
<script>
    async("//cdnjs.cloudflare.com/ajax/libs/anchor-js/1.1.1/anchor.min.js",function(){
        anchors.options = {
          visible: 'always',
          placement: 'right',
          icon: '#'
        };
        anchors.add().remove('.intro-header h1').remove('.subheading').remove('.sidebar-container h5');
    })
</script>
<style>
    /* place left on bigger screen */
    @media all and (min-width: 800px) {
        .anchorjs-link{
            position: absolute;
            left: -0.75em;
            font-size: 1.1em;
            margin-top : -0.1em;
        }
    }
</style>



    <!-- Footer -->
    <!-- Footer -->
<footer>
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <ul class="list-inline text-center">
                
                
                

                

                

                
                    <li>
                        <a target="_blank"  href="https://github.com/Gloomymoon">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                

                

                </ul>
                <p class="copyright text-muted">
                    Copyright &copy; Gloomymoon 2017
                    <br>
                    Theme by <a href="http://huangxuan.me">Hux</a>
                    <span style="display: inline-block; margin: 0 5px;">
                        <i class="fa fa-heart"></i>
                    </span>
                    Ported by <a href="http://blog.kaijun.rocks">Kaijun</a> |
                    <iframe
                        style="margin-left: 2px; margin-bottom:-5px;"
                        frameborder="0" scrolling="0" width="91px" height="20px"
                        src="https://ghbtns.com/github-btn.html?user=kaijun&repo=hexo-theme-huxblog&type=star&count=true" >
                    </iframe>
                </p>
            </div>
        </div>
    </div>
</footer>

<!-- jQuery -->
<script src="/js/jquery.min.js"></script>

<!-- Bootstrap Core JavaScript -->
<script src="/js/bootstrap.min.js"></script>

<!-- Custom Theme JavaScript -->
<script src="/js/hux-blog.min.js"></script>


<!-- async load function -->
<script>
    function async(u, c) {
      var d = document, t = 'script',
          o = d.createElement(t),
          s = d.getElementsByTagName(t)[0];
      o.src = u;
      if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
      s.parentNode.insertBefore(o, s);
    }
</script>

<!--
     Because of the native support for backtick-style fenced code blocks
     right within the Markdown is landed in Github Pages,
     From V1.6, There is no need for Highlight.js,
     so Huxblog drops it officially.

     - https://github.com/blog/2100-github-pages-now-faster-and-simpler-with-jekyll-3-0
     - https://help.github.com/articles/creating-and-highlighting-code-blocks/
-->
<!--
    <script>
        async("http://cdn.bootcss.com/highlight.js/8.6/highlight.min.js", function(){
            hljs.initHighlightingOnLoad();
        })
    </script>
    <link href="http://cdn.bootcss.com/highlight.js/8.6/styles/github.min.css" rel="stylesheet">
-->


<!-- jquery.tagcloud.js -->
<script>
    // only load tagcloud.js in tag.html
    if($('#tag_cloud').length !== 0){
        async("http://gloomymoon.github.io/js/jquery.tagcloud.js",function(){
            $.fn.tagcloud.defaults = {
                //size: {start: 1, end: 1, unit: 'em'},
                color: {start: '#bbbbee', end: '#0085a1'},
            };
            $('#tag_cloud a').tagcloud();
        })
    }
</script>

<!--fastClick.js -->
<script>
    async("//cdnjs.cloudflare.com/ajax/libs/fastclick/1.0.6/fastclick.min.js", function(){
        var $nav = document.querySelector("nav");
        if($nav) FastClick.attach($nav);
    })
</script>


<!-- Google Analytics -->




<!-- Baidu Tongji -->


<!-- Side Catalog -->




<!-- Image to hack wechat -->
<img src="http://gloomymoon.github.io/img/icon_wechat.png" width="0" height="0" />
<!-- Migrate from head to bottom, no longer block render and still work -->

</body>

</html>
