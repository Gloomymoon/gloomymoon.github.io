<!DOCTYPE html>
<html lang="en">

<!-- Head tag -->
<head><meta name="generator" content="Hexo 3.8.0">
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="google-site-verification" content="xBT4GhYoi5qRD5tr338pgPM5OWHHIDR6mNg1a3euekI">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <meta name="keyword" content="">
    <link rel="shortcut icon" href="/img/favicon.ico">

    <title>
        
          Python Machine Learning V - Master Gloomymoon&#39;s R2D2
        
    </title>

    <link rel="canonical" href="http://gloomymoon.github.io/2017/03/19/Python-Machine-Learning-V/">

    <!-- Bootstrap Core CSS -->
    <link rel="stylesheet" href="/css/bootstrap.min.css">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/hux-blog.css">

    <!-- Pygments Highlight CSS -->
    <link rel="stylesheet" href="/css/highlight.css">

    <!-- Custom Fonts -->
    <!-- <link href="http://maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css" rel="stylesheet" type="text/css"> -->
    <!-- Hux change font-awesome CDN to qiniu -->
    <link rel="stylesheet" href="/css/font-awesome.min.css">
    <link href="//cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css">


    <!-- Hux Delete, sad but pending in China
    <link href='http://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
    <link href='http://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800' rel='stylesheet' type='text/
    css'>
    -->


    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

    <!-- ga & ba script hoook -->
    <script></script>
</head>


<!-- hack iOS CSS :active style -->
<body ontouchstart="">

    <!-- Navigation -->
<nav class="navbar navbar-default navbar-custom navbar-fixed-top">
    <div class="container-fluid">
        <!-- Brand and toggle get grouped for better mobile display -->
        <div class="navbar-header page-scroll">
            <button type="button" class="navbar-toggle">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="/">Gloomymoon</a>
        </div>

        <!-- Collect the nav links, forms, and other content for toggling -->
        <!-- Known Issue, found by Hux:
            <nav>'s height woule be hold on by its content.
            so, when navbar scale out, the <nav> will cover tags.
            also mask any touch event of tags, unfortunately.
        -->
        <div id="huxblog_navbar">
            <div class="navbar-collapse">
                <ul class="nav navbar-nav navbar-right">
                    <li>
                        <a href="/">Home</a>
                    </li>

                    

                        
                        <li>
                            <a href="/about/">About</a>
                        </li>
                        
                    

                        
                        <li>
                            <a href="/tags/">Tags</a>
                        </li>
                        
                    

                        
                        <li>
                            <a href="/archives/">Archives</a>
                        </li>
                        
                    

                        
                    
                    
                </ul>
            </div>
        </div>
        <!-- /.navbar-collapse -->
    </div>
    <!-- /.container -->
</nav>
<script>
    // Drop Bootstarp low-performance Navbar
    // Use customize navbar with high-quality material design animation
    // in high-perf jank-free CSS3 implementation
    var $body   = document.body;
    var $toggle = document.querySelector('.navbar-toggle');
    var $navbar = document.querySelector('#huxblog_navbar');
    var $collapse = document.querySelector('.navbar-collapse');

    $toggle.addEventListener('click', handleMagic)
    function handleMagic(e){
        if ($navbar.className.indexOf('in') > 0) {
        // CLOSE
            $navbar.className = " ";
            // wait until animation end.
            setTimeout(function(){
                // prevent frequently toggle
                if($navbar.className.indexOf('in') < 0) {
                    $collapse.style.height = "0px"
                }
            },400)
        }else{
        // OPEN
            $collapse.style.height = "auto"
            $navbar.className += " in";
        }
    }
</script>


    <!-- Main Content -->
    
<!-- Image to hack wechat -->
<!-- <img src="http://gloomymoon.github.io/img/icon_wechat.png" width="0" height="0"> -->
<!-- <img src="{{ site.baseurl }}/{% if page.header-img %}{{ page.header-img }}{% else %}{{ site.header-img }}{% endif %}" width="0" height="0"> -->

<!-- Post Header -->
<style type="text/css">
    header.intro-header{
        background-image: url('/img/star-wars.jpg');
    }
</style>
<header class="intro-header">
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <div class="post-heading">
                    <div class="tags">
                        
                          <a class="tag" href="/tags/#Python" title="Python">Python</a>
                        
                          <a class="tag" href="/tags/#Machine Learning" title="Machine Learning">Machine Learning</a>
                        
                          <a class="tag" href="/tags/#scikit-learn" title="scikit-learn">scikit-learn</a>
                        
                    </div>
                    <h1>Python Machine Learning V</h1>
                    <h2 class="subheading"></h2>
                    <span class="meta">
                        Posted by Gloomymoon on
                        2017-03-19
                    </span>
                </div>
            </div>
        </div>
    </div>
</header>

<!-- Post Content -->
<article>
    <div class="container">
        <div class="row">

    <!-- Post Container -->
            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                post-container">

                <h2 id="5-Compressing-Data-via-Dimensionality-Reduction"><a href="#5-Compressing-Data-via-Dimensionality-Reduction" class="headerlink" title="5 Compressing Data via Dimensionality Reduction"></a>5 Compressing Data via Dimensionality Reduction</h2><p>本章将介绍三类基础方法，将训练数据映射到低维特征子空间。数据压缩是机器学习中一个重要的课题，它帮助我们存储和分析现代科技收集的增长惊人的数据。本章将涵盖如下主要内容：</p>
<ul>
<li>主成分分析（Principal component analysis，PCA），用于无监督数据压缩</li>
<li>线性判别分析（Linear Discriminant Analysis，LDA），作为一种有监督的降维技术最大化区分性</li>
<li>利用核主成分分析法进行非线性降维</li>
</ul>
<h3 id="Unsupervised-dimensionality-reduction-via-principal-component-analysis"><a href="#Unsupervised-dimensionality-reduction-via-principal-component-analysis" class="headerlink" title="Unsupervised dimensionality reduction via principal component analysis"></a>Unsupervised dimensionality reduction via principal component analysis</h3><p>主成分分析（PCA）是一种无监督线性转换技术了，除了维度压缩以外也应用于很多其他领域，例如数据的探索分析、股票交易的信号降噪和生物信息学中的基因组数据分析及基因显性水平。PCA协助我们基于特征间的相关性识别数据中隐含的模式。简言之，PCA旨在找到高维数据中最大方差（variance）的方向并投射到一个有同等或更低维度的子空间上。新子空间中两两正交的轴（也就是主成份principal components)既可以视为方差最大的方向。如下图所示，x1和x2是原始的特征坐标，PC1和PC2即是主成份。子空间的维度是全新构造出来的的正交特征，也称为主元，而不是简单的从原始特征维中去除。PCA的算法原理这里不做介绍了。<br><img src="/img/PythonMachineLearningV_01.png" alt=""><br>PCA算法主要步骤如下：</p>
<ol>
<li>标准化d维的原始数据集</li>
<li>构建协方差矩阵</li>
<li>求解协方差矩阵的特征值（eigenvalues）和特征向量（eigenvactors）</li>
<li>取最大的k个特征值对应的特征向量</li>
<li>将选取的特征向量作为列向量组成投影矩阵w</li>
<li>使用w将样本数据投影到选取的k个特征向量上<blockquote>
<p>可参见<a href="http://www.cnblogs.com/jerrylead/archive/2011/04/18/2020209.html" target="_blank" rel="noopener"></a></p>
</blockquote>
</li>
</ol>
<p><strong>Total and explained variance</strong><br>本节我们先对付PCA算法的前四布：标准化数据、构建协方差矩阵、计算特征值和特征向量、排序特征向量。<br>我们仍使用上一张用到的Wine数据集来举例，并按照70:30的比例分为训练和测试集，然后进行方差为1的标准化。<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">import pandas as pd</span><br><span class="line">df_wine = pd.read_csv(<span class="string">'https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data'</span>, header=None)</span><br><span class="line">from sklearn<span class="selector-class">.cross_validation</span> import train_test_split</span><br><span class="line">from sklearn<span class="selector-class">.preprocessing</span> import StandardScaler</span><br><span class="line">X, y = df_wine<span class="selector-class">.iloc</span>[:, <span class="number">1</span>:]<span class="selector-class">.values</span>, df_wine<span class="selector-class">.iloc</span>[:, <span class="number">0</span>].values</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.3</span>, random_state=<span class="number">0</span>)</span><br><span class="line">sc = StandardScaler()</span><br><span class="line">X_train_std = sc.fit_transform(X_train)</span><br><span class="line">Xtest_std = sc.fit_transform(X_test)</span><br></pre></td></tr></table></figure></p>
<p>第二步是构建一个dxd维的协方差矩阵，d等于数据集中变量的个数，存放两两特征间的协方差，协方差值为正时表示两个变量同时增加后减少，负值表示变量变化趋势相反。我们可以直接使用NumPy中的<code>linalg.eig</code>函数直接计算Wine数据的协方差矩阵：<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">cov_mat = np.cov(X_train_std.T)</span><br><span class="line">eigen_vals, eigen_vecs = np<span class="selector-class">.linalg</span><span class="selector-class">.eig</span>(cov_mat)</span><br><span class="line"><span class="function"><span class="title">print</span><span class="params">(<span class="string">'\nEigenvalues \n%s'</span> % eigen_vals)</span></span></span><br></pre></td></tr></table></figure></p>
<p>Output:<br><figure class="highlight dns"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Eigenvalues </span><br><span class="line">[ <span class="number">4.8923083</span>   <span class="number">2.46635032</span>  <span class="number">1.42809973</span>  <span class="number">1.01233462</span>  <span class="number">0.84906459</span>  <span class="number">0.60181514</span></span><br><span class="line">  <span class="number">0.52251546</span>  <span class="number">0.08414846</span>  <span class="number">0.33051429</span>  <span class="number">0.29595018</span>  <span class="number">0.16831254</span>  <span class="number">0.21432212</span></span><br><span class="line">  <span class="number">0.2399553</span> ]</span><br></pre></td></tr></table></figure></p>
<p><code>numpy.cov</code>方法用于计算标准化后训练数据集的协方差矩阵，<code>linalg.eig</code>方法对协方差矩阵进行特征分解，返回一个含有13个特征值的响亮（eigen_vals）和对应的特征向量（存储为13x13维度的矩阵eigen_vecs）。因为我们希望通过引射到一个新的子空间上达到降低数据维度的目的，因此需要选择包含有最大信息的特征向量（也就是主成份），而特征值就定义了特征向量的重要程度，因此只要将特征值降序排列，提取我们感兴趣的前k个特征向量即可。在这之前我们先来看下每个特征值都赢得方差百分比（variance explained ratios）。每个特征值对应的方差百分比是其自身与所有特征值总和的比值。<br><figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">tot = <span class="built_in">sum</span>(eigen_vals)</span><br><span class="line"><span class="built_in">var_exp</span> = [(i / tot) <span class="keyword">for</span> i <span class="keyword">in</span> sorted(eigen_vals, <span class="built_in">reverse</span>=True)]</span><br><span class="line">cum_var_exp = <span class="built_in">np</span>.cumsum(<span class="built_in">var_exp</span>)</span><br><span class="line"></span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">plt.bar(<span class="built_in">range</span>(<span class="number">1</span>, <span class="number">14</span>), <span class="built_in">var_exp</span>, alpha=<span class="number">0.5</span>, align='<span class="built_in">center</span>', <span class="built_in">label</span>='individual explained variance')</span><br><span class="line">plt.<span class="keyword">step</span>(<span class="built_in">range</span>(<span class="number">1</span>, <span class="number">14</span>), cum_var_exp, where='mid', <span class="built_in">label</span>='cumulative explained variance')</span><br><span class="line">plt.<span class="built_in">ylabel</span>('Explained variance ratio')</span><br><span class="line">plt.<span class="built_in">xlabel</span>('Principal <span class="built_in">components</span>')</span><br><span class="line">plt.<span class="built_in">show</span>()</span><br></pre></td></tr></table></figure></p>
<p><img src="/img/PythonMachineLearningV_02.png" alt=""><br>从结果上看，第一个主成份贡献了近40%的方差，前两个贡献了近60%的方差。<br>虽然方差解释图和上章中通过随机森林的特征选择类似，但是主成分分析是一种无监督的方法，其分析并不基于分类标签，而是衡量在特征轴上数据值的离散情况。</p>
<p><strong>Feature transformation</strong><br>完成协方差矩阵分写后就可以将Wine数据集转换到新的主成份特征向量上。首先是按照特征值降序对特征对排序：<br><figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">eigen_pairs = [(<span class="built_in">np</span>.<span class="built_in">abs</span>(eigen_vals[i]), eigen_vecs[:, i])</span><br><span class="line">               <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(len(eigen_vals))]</span><br><span class="line">eigen_pairs.<span class="built_in">sort</span>(<span class="built_in">reverse</span>=True)</span><br></pre></td></tr></table></figure></p>
<p>然后我们选取值最大的两个特征向量创建映射矩阵。因为为了展示方便，仅选取两个特征变量以便能够在二维散点图上展现，实际工作中主成份的数量需要衡量，综合考虑计算效率和分类器的效果。<br><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">w = np.hstack((eigen_pairs[<span class="string">0</span>][<span class="symbol">1</span>][<span class="string">:, np.newaxis</span>],</span><br><span class="line"><span class="code">                eigen_pairs[1][1][:, np.newaxis]))</span></span><br><span class="line">print('Matrix W:\n', w)</span><br></pre></td></tr></table></figure></p>
<p>Output:<br><figure class="highlight dns"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">Matrix W:</span><br><span class="line"> [[ <span class="number">0.14669811</span>  <span class="number">0.50417079</span>]</span><br><span class="line"> [-<span class="number">0.24224554</span>  <span class="number">0.24216889</span>]</span><br><span class="line"> [-<span class="number">0.02993442</span>  <span class="number">0.28698484</span>]</span><br><span class="line"> [-<span class="number">0.25519002</span> -<span class="number">0.06468718</span>]</span><br><span class="line"> [ <span class="number">0.12079772</span>  <span class="number">0.22995385</span>]</span><br><span class="line"> [ <span class="number">0.38934455</span>  <span class="number">0.09363991</span>]</span><br><span class="line"> [ <span class="number">0.42326486</span>  <span class="number">0.01088622</span>]</span><br><span class="line"> [-<span class="number">0.30634956</span>  <span class="number">0.01870216</span>]</span><br><span class="line"> [ <span class="number">0.30572219</span>  <span class="number">0.03040352</span>]</span><br><span class="line"> [-<span class="number">0.09869191</span>  <span class="number">0.54527081</span>]</span><br><span class="line"> [ <span class="number">0.30032535</span> -<span class="number">0.27924322</span>]</span><br><span class="line"> [ <span class="number">0.36821154</span> -<span class="number">0.174365</span>  ]</span><br><span class="line"> [ <span class="number">0.29259713</span>  <span class="number">0.36315461</span>]]</span><br></pre></td></tr></table></figure></p>
<p>在原始数据上对每一条记录进行转换（点积）就能够转换成2个新的特征，同样对所有数据应用映射也可以直接通过点积操作。最后我们通过二维散点图看下转换今后的数据分布情况。<br><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">X_train_pca = X_train_std.<span class="built_in">dot</span>(w)</span><br><span class="line">colors = [<span class="string">'r'</span>, <span class="string">'b'</span>, <span class="string">'g'</span>]</span><br><span class="line">markers = [<span class="string">'s'</span>, <span class="string">'x'</span>, <span class="string">'o'</span>]</span><br><span class="line"><span class="keyword">for</span> l, c, m in zip(np.unique(y_train), colors, markers):</span><br><span class="line">    plt.<span class="built_in">scatter</span>(X_train_pca[y_train==l, <span class="number">0</span>],</span><br><span class="line">                X_train_pca[y_train==l, <span class="number">1</span>],</span><br><span class="line">                c=c, label=l, marker=m)</span><br><span class="line">plt.xlabel(<span class="string">'PC 1'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'PC 2'</span>)</span><br><span class="line">plt.<span class="built_in">legend</span>(loc=<span class="string">'lower left'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p>
<p><img src="/img/PythonMachineLearningV_03.png" alt=""><br>转换后的数据在X轴（第一个主成份）上的分布比Y轴（第二个主成份）上区分度更好，通过一个简单的线性分类器就能够很好地定义数据分布界限。</p>
<p><strong>Principal component analysis in scikit-learn</strong><br>scikit-learn中已经实现的PCA类是一个数据变化模块，和之前的类似，优先使用<code>fit</code>方法在训练数据上学习，然后转换训练数据和测试数据。下面我们结合PCA的转换、逻辑回归分类建模，并将模型结果用第二章中的方法展现出来：<br><figure class="highlight nix"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">from plot_decision_regions <span class="built_in">import</span> *</span><br><span class="line">from sklearn.linear_model <span class="built_in">import</span> LogisticRegression</span><br><span class="line">from sklearn.decomposition <span class="built_in">import</span> PCA</span><br><span class="line"><span class="attr">pca</span> = PCA(<span class="attr">n_components=2)</span></span><br><span class="line"><span class="attr">lr</span> = LogisticRegression()</span><br><span class="line"><span class="attr">X_train_pca</span> = pca.fit_transform(X_train_std)</span><br><span class="line"><span class="attr">X_test_pca</span> = pca.transform(X_test_std)</span><br><span class="line">lr.fit(X_train_pca, y_train)</span><br><span class="line">plot_decision_regions(X_train_pca, y_train, <span class="attr">classifier=lr)</span></span><br><span class="line">plt.xlabel('PC <span class="number">1</span>')</span><br><span class="line">plt.ylabel('PC <span class="number">2</span>')</span><br><span class="line">plt.legend(<span class="attr">loc='lower</span> left')</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p>
<p><img src="/img/PythonMachineLearningV_04.png" alt=""><br>注意到这个散点图和我们自己实现的PCA不同，是X轴的镜像，这不是算法的错误，而是因为特征向量可正可负，我们可以简单的对数据乘以-1，使得结果一样。最后我们将逻辑归回模型应用在转换后的测试集上看下分类效果：<br><figure class="highlight less"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">plot_decision_regions</span>(X_test_pca, y_test, classifier=lr)</span><br><span class="line"><span class="selector-tag">plt</span><span class="selector-class">.xlabel</span>(<span class="string">'PC 1'</span>)</span><br><span class="line"><span class="selector-tag">plt</span><span class="selector-class">.ylabel</span>(<span class="string">'PC 2'</span>)</span><br><span class="line"><span class="selector-tag">plt</span><span class="selector-class">.legend</span>(loc=<span class="string">'lower left'</span>)</span><br><span class="line"><span class="selector-tag">plt</span><span class="selector-class">.show</span>()</span><br></pre></td></tr></table></figure></p>
<p><img src="/img/PythonMachineLearningV_05.png" alt=""><br>简单的逻辑回归在这个2维的特征子空间上表现不错，仅有1个样本被错误分类。</p>
<p>如果我们需要了解不同主成份的方差比例，可以在初始化PCA类的时候将参数<code>n_components</code>设置为<code>None</code>，这样所有的主成份都会保留，方差比例可以通过<code>explained_variance_ratio_</code>属性访问。<br><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pca = PCA(n_components=None)</span><br><span class="line">X_train_pca = pca.fit_transform(X_train_std)</span><br><span class="line">pca.explained_variance_ratio_</span><br></pre></td></tr></table></figure></p>
<p>Output:<br><figure class="highlight dns"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">array([ <span class="number">0.37329648</span>,  <span class="number">0.18818926</span>,  <span class="number">0.10896791</span>,  <span class="number">0.07724389</span>,  <span class="number">0.06478595</span>,</span><br><span class="line">        <span class="number">0.04592014</span>,  <span class="number">0.03986936</span>,  <span class="number">0.02521914</span>,  <span class="number">0.02258181</span>,  <span class="number">0.01830924</span>,</span><br><span class="line">        <span class="number">0.01635336</span>,  <span class="number">0.01284271</span>,  <span class="number">0.00642076</span>])</span><br></pre></td></tr></table></figure></p>
<h3 id="Supervised-data-compression-via-linear-discriminant-analysis"><a href="#Supervised-data-compression-via-linear-discriminant-analysis" class="headerlink" title="Supervised data compression via linear discriminant analysis"></a>Supervised data compression via linear discriminant analysis</h3><p>线性判别分析（Linear Discriminant Analysis，LDA）是一种特征压缩技术，可以提升模型计算效率降低过拟合。和PCA背后的原理类似，LDA的目标是搜索一个分类分布更优的特征子空间。PCA和LDA都是通过线性转换技术降低数据的维度，前者是无监督算法，而后者是有监督的。直觉上有监督学习效果优于无监督学习，但是实际上在某些领域例如特定图像识别问题上，PCA反而有更加出色的表现。</p>
<p>LDA的主要步骤如下：</p>
<ol>
<li>标准化d维数据（d表示特征的数量）</li>
<li>为每个分类计算d维的均值向量（mean vector）</li>
<li>构建类间散布矩阵Sb（between-class scatter matrix）和类内散布矩阵Sw（within-class scatter matrix）</li>
<li>计算(Sw^-1)(Sb)的特征向量和特征值</li>
<li>选择k个最大特征值对应的特征向量，组成d×k维的转换矩阵W，特征向量是矩阵的列</li>
<li>将样本通过W投射到新的特征子空间上</li>
</ol>
<p><strong>Computing the scatter matrices</strong><br>因为在之前PCA步骤中我们已经标准化了Wine数据集，这里不再重复。我们首先为每个分类计算均值向量：<br><figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">np</span>.set_printoptions(precision=<span class="number">4</span>)</span><br><span class="line">mean_vecs = []</span><br><span class="line"><span class="keyword">for</span> <span class="built_in">label</span> <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">4</span>):</span><br><span class="line">    mean_vecs.<span class="built_in">append</span>(<span class="built_in">np</span>.<span class="built_in">mean</span>(</span><br><span class="line">                     X_train_std[y_train==<span class="built_in">label</span>], axis=<span class="number">0</span>))</span><br><span class="line">    <span class="built_in">print</span>('MV <span class="built_in">%s</span>: <span class="built_in">%s</span>\n' <span class="symbol">%</span>(<span class="built_in">label</span>, mean_vecs[<span class="built_in">label</span>-<span class="number">1</span>]))</span><br></pre></td></tr></table></figure></p>
<p>Output:<br><figure class="highlight subunit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">MV 1: [ 0.9259 <span class="string">-0</span>.3091  0.2592 <span class="string">-0</span>.7989  0.3039  0.9608  1.0515 <span class="string">-0</span>.6306  0.5354</span><br><span class="line">  0.2209  0.4855  0.798   1.2017]</span><br><span class="line"></span><br><span class="line">MV 2: [<span class="string">-0</span>.8727 <span class="string">-0</span>.3854 <span class="string">-0</span>.4437  0.2481 <span class="string">-0</span>.2409 <span class="string">-0</span>.1059  0.0187 <span class="string">-0</span>.0164  0.1095</span><br><span class="line"> <span class="string">-0</span>.8796  0.4392  0.2776 <span class="string">-0</span>.7016]</span><br><span class="line"></span><br><span class="line">MV 3: [ 0.1637  0.8929  0.3249  0.5658 <span class="string">-0</span>.01   <span class="string">-0</span>.9499 <span class="string">-1</span>.228   0.7436 <span class="string">-0</span>.7652</span><br><span class="line">  0.979  <span class="string">-1</span>.1698 <span class="string">-1</span>.3007 <span class="string">-0</span>.3912]</span><br></pre></td></tr></table></figure></p>
<p>有了均值向量后我们可以用来计算类内散步矩阵Sw：<br><img src="/img/PythonMachineLearningV_06.png" alt=""><br>其中i代表分类，m是分类的均值向量，x是特征。<br><figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">d = <span class="number">13</span> # number of <span class="built_in">features</span></span><br><span class="line">S_W = <span class="built_in">np</span>.zeros((d, d))</span><br><span class="line"><span class="keyword">for</span> <span class="built_in">label</span>, mv <span class="keyword">in</span> zip(<span class="built_in">range</span>(<span class="number">1</span>, <span class="number">4</span>), mean_vecs):</span><br><span class="line">    class_scatter = <span class="built_in">np</span>.zeros((d, d))</span><br><span class="line">    <span class="keyword">for</span> <span class="built_in">row</span> <span class="keyword">in</span> X[y == <span class="built_in">label</span>]:</span><br><span class="line">        <span class="built_in">row</span>, mv = <span class="built_in">row</span>.reshape(d, <span class="number">1</span>), mv.reshape(d, <span class="number">1</span>)</span><br><span class="line">        class_scatter += (<span class="built_in">row</span>-mv).dot((<span class="built_in">row</span>-mv).T)</span><br><span class="line">    S_W += class_scatter</span><br><span class="line"><span class="built_in">print</span>('Within-class scatter <span class="built_in">matrix</span>: %sx%s' <span class="symbol">%</span> (S_W.shape[<span class="number">0</span>], S_W.shape[<span class="number">1</span>]))</span><br></pre></td></tr></table></figure></p>
<p>Output:<br><figure class="highlight kotlin"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Within-<span class="class"><span class="keyword">class</span> <span class="title">scatter</span> <span class="title">matrix</span>: <span class="type">13x13</span></span></span><br></pre></td></tr></table></figure></p>
<p>由于我们在构建散步矩阵前假设训练集中的每个分类数量是均匀分布的，但是实际情况中，不可能这样完美：<br><figure class="highlight lisp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print('Class label distribution: %s' % np.bincount(<span class="name">y_train</span>)[<span class="number">1</span>:])</span><br></pre></td></tr></table></figure></p>
<p>Output:<br><figure class="highlight delphi"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">Class</span> <span class="keyword">label</span> distribution: [<span class="number">40</span> <span class="number">49</span> <span class="number">35</span>]</span><br></pre></td></tr></table></figure></p>
<p>因此我们在汇总所有Si前需要做归一化处理，对每个Si都要除以分类i中样本的个数，这样有点类似于我们之前的协方差矩阵：<br><figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">d = <span class="number">13</span> # number of <span class="built_in">features</span></span><br><span class="line">S_W = <span class="built_in">np</span>.zeros((d, d))</span><br><span class="line"><span class="keyword">for</span> <span class="built_in">label</span>, mv <span class="keyword">in</span> zip(<span class="built_in">range</span>(<span class="number">1</span>, <span class="number">4</span>), mean_vecs):</span><br><span class="line">    class_scatter = <span class="built_in">np</span>.<span class="built_in">cov</span>(X_train_std[y_train==<span class="built_in">label</span>].T)</span><br><span class="line">    S_W += class_scatter</span><br><span class="line"><span class="built_in">print</span>('Within-class scatter <span class="built_in">matrix</span>: %sx%s' <span class="symbol">%</span> (S_W.shape[<span class="number">0</span>], S_W.shape[<span class="number">1</span>]))</span><br></pre></td></tr></table></figure></p>
<p>然后可以构建类间散布矩阵Sb：<br><img src="/img/PythonMachineLearningV_07.png" alt=""><br>其中m是总体的均值向量。<br><figure class="highlight nix"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">mean_overall</span> = np.mean(X_train_std, <span class="attr">axis=0)</span></span><br><span class="line"><span class="attr">d</span> = <span class="number">13</span> <span class="comment"># number of features</span></span><br><span class="line"><span class="attr">S_B</span> = np.zeros((d, d))</span><br><span class="line">for i, mean_vec <span class="keyword">in</span> enumerate(mean_vecs):</span><br><span class="line">    <span class="attr">n</span> = X[<span class="attr">y==i+1,</span> :].shape[<span class="number">0</span>]</span><br><span class="line">    <span class="attr">mean_vec</span> = mean_vec.reshape(d, <span class="number">1</span>)</span><br><span class="line">    <span class="attr">mean_overall</span> = mean_overall.reshape(d, <span class="number">1</span>)</span><br><span class="line">    S_B += n * (mean_vec - mean_overall).dot((mean_vec - mean_overall).T)</span><br><span class="line">print('Between-class scatter matrix: %sx%s' % (S_B.shape[<span class="number">0</span>], S_B.shape[<span class="number">1</span>]))</span><br></pre></td></tr></table></figure></p>
<p><strong>Selecting linear discrimnants for the new feature subspace</strong><br>接下来的工作与PCA类似：<br><figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">eigen_vals, eigen_vecs = <span class="built_in">np</span>.linalg.eig(<span class="built_in">np</span>.linalg.inv(S_W).dot(S_B))</span><br><span class="line">eigen_pairs = [(<span class="built_in">np</span>.<span class="built_in">abs</span>(eigen_vals[i]), eigen_vecs[:, i]) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(len(eigen_vals))]</span><br><span class="line">eigen_pairs = sorted(eigen_pairs, <span class="built_in">key</span>=<span class="built_in">lambda</span> k: k[<span class="number">0</span>], <span class="built_in">reverse</span>=True)</span><br><span class="line"><span class="built_in">print</span>('EigenValues <span class="keyword">in</span> <span class="built_in">decreasing</span> order:\n')</span><br><span class="line"><span class="keyword">for</span> eigen_val <span class="keyword">in</span> eigen_pairs:</span><br><span class="line">    <span class="built_in">print</span>(eigen_val[<span class="number">0</span>])</span><br><span class="line">`</span><br></pre></td></tr></table></figure></p>
<p>降序排列的特征值如下：<br><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">EigenValues</span> <span class="selector-tag">in</span> <span class="selector-tag">decreasing</span> <span class="selector-tag">order</span>:</span><br><span class="line"></span><br><span class="line">643<span class="selector-class">.015384346</span></span><br><span class="line">225<span class="selector-class">.086981854</span></span><br><span class="line">8<span class="selector-class">.00267518379e-14</span></span><br><span class="line">5<span class="selector-class">.75753461418e-14</span></span><br><span class="line">3<span class="selector-class">.51050796047e-14</span></span><br><span class="line">3<span class="selector-class">.46389583683e-14</span></span><br><span class="line">2<span class="selector-class">.58781151001e-14</span></span><br><span class="line">2<span class="selector-class">.58781151001e-14</span></span><br><span class="line">2<span class="selector-class">.44498173106e-14</span></span><br><span class="line">1<span class="selector-class">.65321991297e-14</span></span><br><span class="line">8<span class="selector-class">.33122517135e-15</span></span><br><span class="line">2<span class="selector-class">.3238388797e-15</span></span><br><span class="line">6<span class="selector-class">.52243007612e-16</span></span><br></pre></td></tr></table></figure></p>
<p>熟悉线性代数的同学或许还记得一个d×d维的协方差矩阵的秩的数量不会超过d-1个，这里可以看到仅有两个非零的特征值。<br>我们同样用类似PCA中方差百分比的贡献度，将片别分析中每个分类判别的信息都通过图表来展现：<br><figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">tot = <span class="built_in">sum</span>(eigen_vals.<span class="built_in">real</span>)</span><br><span class="line">discr = [(i / tot) <span class="keyword">for</span> i <span class="keyword">in</span> sorted(eigen_vals.<span class="built_in">real</span>, <span class="built_in">reverse</span>=True)]</span><br><span class="line">cum_discr = <span class="built_in">np</span>.cumsum(discr)</span><br><span class="line">plt.bar(<span class="built_in">range</span>(<span class="number">1</span>, <span class="number">14</span>), discr, alpha=<span class="number">0.5</span>, align='<span class="built_in">center</span>', <span class="built_in">label</span>='individual <span class="string">"discriminability"</span>')</span><br><span class="line">plt.<span class="keyword">step</span>(<span class="built_in">range</span>(<span class="number">1</span>, <span class="number">14</span>), cum_discr, where='mid', <span class="built_in">label</span>='cumulative <span class="string">"discriminability"</span>')</span><br><span class="line">plt.<span class="built_in">ylabel</span>('<span class="string">"discriminability"</span> ratio')</span><br><span class="line">plt.<span class="built_in">xlabel</span>('Linear Discriminants')</span><br><span class="line">plt.ylim([-<span class="number">0.1</span>, <span class="number">1.1</span>])</span><br><span class="line">plt.<span class="built_in">legend</span>(loc='best')</span><br><span class="line">plt.<span class="built_in">show</span>()</span><br></pre></td></tr></table></figure></p>
<p><img src="/img/PythonMachineLearningV_08.png" alt=""><br>前两个线性判别式捕获了将近100%的有用分类信息。然后我们可以用这两个特征变量来组建转换矩阵W：<br><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">w = np.hstack((eigen_pairs[<span class="string">0</span>][<span class="symbol">1</span>][<span class="string">:, np.newaxis</span>].real,</span><br><span class="line"><span class="code">               eigen_pairs[1][1][:, np.newaxis].real))</span></span><br><span class="line">print('Matrix W:\n', w)</span><br></pre></td></tr></table></figure></p>
<p>Output:<br><figure class="highlight subunit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">Matrix W:</span><br><span class="line"> [[<span class="string">-0</span>.0707  0.3778]</span><br><span class="line"> [ 0.0359  0.2223]</span><br><span class="line"> [<span class="string">-0</span>.0263  0.3813]</span><br><span class="line"> [ 0.1875 <span class="string">-0</span>.2955]</span><br><span class="line"> [<span class="string">-0</span>.0033 <span class="string">-0</span>.0143]</span><br><span class="line"> [ 0.2328 <span class="string">-0</span>.0151]</span><br><span class="line"> [<span class="string">-0</span>.7719 <span class="string">-0</span>.2149]</span><br><span class="line"> [<span class="string">-0</span>.0803 <span class="string">-0</span>.0726]</span><br><span class="line"> [ 0.0896 <span class="string">-0</span>.1767]</span><br><span class="line"> [ 0.1815  0.2909]</span><br><span class="line"> [<span class="string">-0</span>.0631 <span class="string">-0</span>.2376]</span><br><span class="line"> [<span class="string">-0</span>.3794 <span class="string">-0</span>.0867]</span><br><span class="line"> [<span class="string">-0</span>.3355  0.586 ]]</span><br></pre></td></tr></table></figure></p>
<p><strong>Projecting samples onto the new feature space</strong><br>类似的，利用转换矩阵将训练数据投影到新的特征子空间上：<br><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">X_train_lda = X_train_std.<span class="built_in">dot</span>(w)</span><br><span class="line">colors = [<span class="string">'r'</span>, <span class="string">'b'</span>, <span class="string">'g'</span>]</span><br><span class="line">markers = [<span class="string">'s'</span>, <span class="string">'x'</span>, <span class="string">'o'</span>]</span><br><span class="line"><span class="keyword">for</span> l, c, m in zip(np.unique(y_train), colors, markers):</span><br><span class="line">    plt.<span class="built_in">scatter</span>(X_train_lda[y_train==l, <span class="number">0</span>],</span><br><span class="line">                X_train_lda[y_train==l, <span class="number">1</span>],</span><br><span class="line">                c=c, label=l, marker=m)</span><br><span class="line">plt.xlabel(<span class="string">'LD 1'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'LD 2'</span>)</span><br><span class="line">plt.<span class="built_in">legend</span>(loc=<span class="string">'upper right'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p>
<p><img src="/img/PythonMachineLearningV_09.png" alt=""></p>
<p><strong>LDA via scikit-learn</strong><br>逐步实现能够帮助我们理解LDA和PCA的内部实现差异，下面我们来看看scikit-learn自带的LDA类对数据降维，然后用逻辑回归来建模的效果；<br><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">X_train_lda = X_train_std.<span class="built_in">dot</span>(w)</span><br><span class="line">colors = [<span class="string">'r'</span>, <span class="string">'b'</span>, <span class="string">'g'</span>]</span><br><span class="line">markers = [<span class="string">'s'</span>, <span class="string">'x'</span>, <span class="string">'o'</span>]</span><br><span class="line"><span class="keyword">for</span> l, c, m in zip(np.unique(y_train), colors, markers):</span><br><span class="line">    plt.<span class="built_in">scatter</span>(X_train_lda[y_train==l, <span class="number">0</span>],</span><br><span class="line">                X_train_lda[y_train==l, <span class="number">1</span>],</span><br><span class="line">                c=c, label=l, marker=m)</span><br><span class="line">plt.xlabel(<span class="string">'LD 1'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'LD 2'</span>)</span><br><span class="line">plt.<span class="built_in">legend</span>(loc=<span class="string">'upper right'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p>
<p><img src="/img/PythonMachineLearningV_10.png" alt=""><br>可以看到逻辑回归只有在一个样本上分类错误。但是通过降低正则化强度，我们能够适当调整分类面使得模型能够对所有数据正确分类。不过，我们先来看下模型在测试数据集上的表现情况：<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">X_test_lda = lda.<span class="attribute">transform</span>(X_test_std)</span><br><span class="line"><span class="function"><span class="title">plot_decision_regions</span><span class="params">(X_test_lda, y_test, classifier=lr)</span></span></span><br><span class="line">plt.xlabel(<span class="string">'LD 1'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'LD 2'</span>)</span><br><span class="line">plt.legend(loc=<span class="string">'lower left'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p>
<p><img src="/img/PythonMachineLearningV_11.png" alt=""><br>Bingo! 模型在测试集上100%正确，仅仅使用了2个特征向量而不是原始13个特征。</p>
<h3 id="Using-Kernel-principal-component-analysis-for-nonlinear-mappings"><a href="#Using-Kernel-principal-component-analysis-for-nonlinear-mappings" class="headerlink" title="Using Kernel principal component analysis for nonlinear mappings"></a>Using Kernel principal component analysis for nonlinear mappings</h3><p>我们之前学习到的大部分机器学习算法都假设输入数据是线性分离的，而现实中我们可能遇到的更多的是非线性分类问题，此时类似PCA和LDA等线性转换技术来降维就不是一个好的选择。本节我们来看下核主成分分析（kernel PCA），将非线性分割问题转换到一个可以线性分类的低维子空间上。<br><img src="/img/PythonMachineLearningV_12.png" alt=""></p>
<p><strong>Kernel functions and the kernel trick</strong><br>还记得我们在第三章介绍的核支持向量机，可以将非线性问题投射到更高维的线性空间上使其转化为线性可分。为了将样本数据转换到更高维的k维子空间上，我们定义了一个非线性映射函数Φ：<br><img src="/img/PythonMachineLearningV_13.png" alt=""><br>例如，如果x是一个二维向量，那么下面就是一种将它投射到三位空间上的一个方法：<br><img src="/img/PythonMachineLearningV_14.png" alt=""><br>结合PCA，我们可以先把一个非线性可分的数据集投射到高维空间上，然后再通过PCA降维到另一个可线性分割的子空间上。这里的难点是，计算非常大，因此我们引入了核机制（kernel trick）。使用核机制可以在原有特征空间上计算两个高位特征向量间的相似度（similarity）。</p>
<p>主要是用到的和函数如下：</p>
<ul>
<li>多项式核函数（the polynomial kernel）</li>
<li>双曲正切函数（the hyperbolic tangent）</li>
<li>高斯和函数或称为径向基函数（Gaussian kernel or Radial Basis Fucntion , RBF）</li>
</ul>
<p><strong>Implementing a kernel principal component analysis in Python</strong><br>略</p>
<p><strong>Projecting new data points</strong><br>略</p>
<p><strong>Kernelprincipal components analysis in scikit-learn</strong><br>scikit-learn在<code>sklearn.decomposion</code>模块中实现了一个核PCA类，使用方法类似标准PCA类，我们可以通过参数<code>kernel</code>自定义和函数：<br><figure class="highlight nix"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.datasets <span class="built_in">import</span> make_moons</span><br><span class="line">from sklearn.decomposition <span class="built_in">import</span> KernelPCA</span><br><span class="line">X, <span class="attr">y</span> = make_moons(<span class="attr">n_samples=100,</span> <span class="attr">random_state=123)</span></span><br><span class="line"><span class="attr">scikit_kpca</span> = KernelPCA(<span class="attr">n_components=2,</span> <span class="attr">kernel='rbf',</span> <span class="attr">gamma=15)</span></span><br><span class="line"><span class="attr">X_skernpca</span> = scikit_kpca.fit_transform(X)</span><br><span class="line"></span><br><span class="line">plt.scatter(X_skernpca[<span class="attr">y==0,</span> <span class="number">0</span>], X_skernpca[<span class="attr">y==0,</span> <span class="number">1</span>], <span class="attr">color='red',</span> <span class="attr">marker='^',</span> <span class="attr">alpha=0.5)</span></span><br><span class="line">plt.scatter(X_skernpca[<span class="attr">y==1,</span> <span class="number">0</span>], X_skernpca[<span class="attr">y==1,</span> <span class="number">1</span>], <span class="attr">color='blue',</span> <span class="attr">marker='o',</span> <span class="attr">alpha=0.5)</span></span><br><span class="line">plt.xlabel('PC <span class="number">1</span>')</span><br><span class="line">plt.ylabel('PC <span class="number">2</span>')</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p>
<p><img src="/img/PythonMachineLearningV_15.png" alt=""></p>


                <hr>

                

                <ul class="pager">
                    
                        <li class="previous">
                            <a href="/2017/03/22/Python-Machine-Learning-VI/" data-toggle="tooltip" data-placement="top" title="Python Machine Learning VI">&larr; Previous Post</a>
                        </li>
                    
                    
                        <li class="next">
                            <a href="/2017/03/15/Bit-City/" data-toggle="tooltip" data-placement="top" title="Bit City">Next Post &rarr;</a>
                        </li>
                    
                </ul>

                

                

            </div>
    <!-- Side Catalog Container -->
        

    <!-- Sidebar Container -->

            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                sidebar-container">

                <!-- Featured Tags -->
                
                <section>
                    <!-- no hr -->
                    <h5><a href="/tags/">FEATURED TAGS</a></h5>
                    <div class="tags">
                       
                          <a class="tag" href="/tags/#Python" title="Python">Python</a>
                        
                          <a class="tag" href="/tags/#Machine Learning" title="Machine Learning">Machine Learning</a>
                        
                          <a class="tag" href="/tags/#scikit-learn" title="scikit-learn">scikit-learn</a>
                        
                    </div>
                </section>
                

                <!-- Friends Blog -->
                
            </div>

        </div>
    </div>
</article>







<!-- async load function -->
<script>
    function async(u, c) {
      var d = document, t = 'script',
          o = d.createElement(t),
          s = d.getElementsByTagName(t)[0];
      o.src = u;
      if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
      s.parentNode.insertBefore(o, s);
    }
</script>
<!-- anchor-js, Doc:http://bryanbraun.github.io/anchorjs/ -->
<script>
    async("//cdnjs.cloudflare.com/ajax/libs/anchor-js/1.1.1/anchor.min.js",function(){
        anchors.options = {
          visible: 'always',
          placement: 'right',
          icon: '#'
        };
        anchors.add().remove('.intro-header h1').remove('.subheading').remove('.sidebar-container h5');
    })
</script>
<style>
    /* place left on bigger screen */
    @media all and (min-width: 800px) {
        .anchorjs-link{
            position: absolute;
            left: -0.75em;
            font-size: 1.1em;
            margin-top : -0.1em;
        }
    }
</style>



    <!-- Footer -->
    <!-- Footer -->
<footer>
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <ul class="list-inline text-center">
                
                
                

                

                

                
                    <li>
                        <a target="_blank" href="https://github.com/Gloomymoon">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                

                

                </ul>
                <p class="copyright text-muted">
                    Copyright &copy; Gloomymoon 2021
                    <br>
                    Theme by <a href="http://huangxuan.me">Hux</a>
                    <span style="display: inline-block; margin: 0 5px;">
                        <i class="fa fa-heart"></i>
                    </span>
                    Ported by <a href="http://blog.kaijun.rocks">Kaijun</a> |
                    <iframe style="margin-left: 2px; margin-bottom:-5px;" frameborder="0" scrolling="0" width="91px" height="20px" src="https://ghbtns.com/github-btn.html?user=kaijun&repo=hexo-theme-huxblog&type=star&count=true">
                    </iframe>
                </p>
            </div>
        </div>
    </div>
</footer>

<!-- jQuery -->
<script src="/js/jquery.min.js"></script>

<!-- Bootstrap Core JavaScript -->
<script src="/js/bootstrap.min.js"></script>

<!-- Custom Theme JavaScript -->
<script src="/js/hux-blog.min.js"></script>


<!-- async load function -->
<script>
    function async(u, c) {
      var d = document, t = 'script',
          o = d.createElement(t),
          s = d.getElementsByTagName(t)[0];
      o.src = u;
      if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
      s.parentNode.insertBefore(o, s);
    }
</script>

<!--
     Because of the native support for backtick-style fenced code blocks
     right within the Markdown is landed in Github Pages,
     From V1.6, There is no need for Highlight.js,
     so Huxblog drops it officially.

     - https://github.com/blog/2100-github-pages-now-faster-and-simpler-with-jekyll-3-0
     - https://help.github.com/articles/creating-and-highlighting-code-blocks/
-->
<!--
    <script>
        async("http://cdn.bootcss.com/highlight.js/8.6/highlight.min.js", function(){
            hljs.initHighlightingOnLoad();
        })
    </script>
    <link href="http://cdn.bootcss.com/highlight.js/8.6/styles/github.min.css" rel="stylesheet">
-->


<!-- jquery.tagcloud.js -->
<script>
    // only load tagcloud.js in tag.html
    if($('#tag_cloud').length !== 0){
        async("http://gloomymoon.github.io/js/jquery.tagcloud.js",function(){
            $.fn.tagcloud.defaults = {
                //size: {start: 1, end: 1, unit: 'em'},
                color: {start: '#bbbbee', end: '#0085a1'},
            };
            $('#tag_cloud a').tagcloud();
        })
    }
</script>

<!--fastClick.js -->
<script>
    async("//cdnjs.cloudflare.com/ajax/libs/fastclick/1.0.6/fastclick.min.js", function(){
        var $nav = document.querySelector("nav");
        if($nav) FastClick.attach($nav);
    })
</script>


<!-- Google Analytics -->




<!-- Baidu Tongji -->


<!-- Side Catalog -->




<!-- Image to hack wechat -->
<img src="http://gloomymoon.github.io/img/icon_wechat.png" width="0" height="0" />
<!-- Migrate from head to bottom, no longer block render and still work -->

</body>

</html>
